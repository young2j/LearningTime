{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 经典又兼具备趣味性的Kaggle案例[泰坦尼克号问题](https://www.kaggle.com/c/titanic)\n",
    "大家都熟悉的『Jack and Rose』的故事，豪华游艇倒了，大家都惊恐逃生，可是救生艇的数量有限，无法人人都有，副船长发话了『lady and kid first！』，所以是否获救其实并非随机，而是基于一些背景有rank先后的。<br>\n",
    "训练和测试数据是一些乘客的个人信息以及存活状况，要尝试根据它生成合适的模型并预测其他人的存活状况。<br>\n",
    "对，这是一个二分类问题，很多分类算法都可以解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>看看数据长什么样</font>**<br>\n",
    "还是用pandas加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个ipython notebook主要是我解决Kaggle Titanic问题的思路和过程\n",
    "\n",
    "import pandas as pd #数据分析\n",
    "import numpy as np #科学计算\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "data_train = pd.read_csv(\"Train.csv\")\n",
    "data_train.columns\n",
    "#data_train[data_train.Cabin.notnull()]['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>我们看大概有以下这些字段</font>**<br>\n",
    "PassengerId => 乘客ID<br>\n",
    "Pclass => 乘客等级(1/2/3等舱位)<br>\n",
    "Name => 乘客姓名<br>\n",
    "Sex => 性别<br>\n",
    "Age => 年龄<br>\n",
    "SibSp => 堂兄弟/妹个数<br>\n",
    "Parch => 父母与小孩个数<br>\n",
    "Ticket => 船票信息<br>\n",
    "Fare => 票价<br>\n",
    "Cabin => 客舱<br>\n",
    "Embarked => 登船港口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>我这么懒的人显然会让pandas自己先告诉我们一些信息<font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>上面的数据说啥了？它告诉我们，训练数据中总共有891名乘客，但是很不幸，我们有些属性的数据不全，比如说：<font><br>\n",
    "\n",
    "* <font color=red>Age（年龄）属性只有714名乘客有记录<font>\n",
    "* <font color=red>Cabin（客舱）更是只有204名乘客是已知的<font>\n",
    "\n",
    "<font color=red>似乎信息略少啊，想再瞄一眼具体数据数值情况呢？恩，我们用下列的方法，得到数值型数据的一些分布(因为有些属性，比如姓名，是文本型；而另外一些属性，比如登船港口，是类目型。这些我们用下面的函数是看不到的)<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>mean字段告诉我们，大概0.383838的人最后获救了，2/3等舱的人数比1等舱要多，平均乘客年龄大概是29.7岁(计算这个时候会略掉无记录的)等等…<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "\n",
    "<font color=red>口号喊完了，上面的简单描述信息并没有什么卵用啊，咱们得再细一点分析下数据啊。<font><br>\n",
    "<font color=red>看看**每个/多个 属性和最后的Survived**之间有着什么样的关系<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0))             # 在一张大图里分列几个小图\n",
    "data_train.Survived.value_counts().plot(kind='bar')# plots a bar graph of those who surived vs those who did not. \n",
    "plt.title(u\"获救情况 (1为获救)\") # puts a title on our graph\n",
    "plt.ylabel(u\"人数\")  \n",
    "\n",
    "plt.subplot2grid((2,3),(0,1))\n",
    "data_train.Pclass.value_counts().plot(kind=\"bar\")\n",
    "plt.ylabel(u\"人数\")\n",
    "plt.title(u\"乘客等级分布\")\n",
    "\n",
    "plt.subplot2grid((2,3),(0,2))\n",
    "plt.scatter(data_train.Survived, data_train.Age)\n",
    "plt.ylabel(u\"年龄\")                         # sets the y axis lable\n",
    "plt.grid(b=True, which='major', axis='y') # formats the grid line style of our graphs\n",
    "plt.title(u\"按年龄看获救分布 (1为获救)\")\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(1,0), colspan=2)\n",
    "data_train.Age[data_train.Pclass == 1].plot(kind='kde')   # plots a kernel desnsity estimate of the subset of the 1st class passanges's age\n",
    "data_train.Age[data_train.Pclass == 2].plot(kind='kde')\n",
    "data_train.Age[data_train.Pclass == 3].plot(kind='kde')\n",
    "plt.xlabel(u\"年龄\")# plots an axis lable\n",
    "plt.ylabel(u\"密度\") \n",
    "plt.title(u\"各等级的乘客年龄分布\")\n",
    "plt.legend((u'头等舱', u'2等舱',u'3等舱'),loc='best') # sets our legend for our graph.\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(1,2))\n",
    "data_train.Embarked.value_counts().plot(kind='bar')\n",
    "plt.title(u\"各登船口岸上船人数\")\n",
    "plt.ylabel(u\"人数\")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是得到了像下面这样一张图：<br>\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>bingo，图还是比数字好看多了。所以我们在图上可以看出来:<font><br>\n",
    "* <font color=red>被救的人300多点，不到半数；<font><br>\n",
    "* <font color=red>3等舱乘客灰常多；遇难和获救的人年龄似乎跨度都很广；<font><br>\n",
    "* <font color=red>3个不同的舱年龄总体趋势似乎也一致，2/3等舱乘客20岁多点的人最多，1等舱40岁左右的最多(→_→似乎符合财富和年龄的分配哈，咳咳，别理我，我瞎扯的)；<font><br>\n",
    "* <font color=red>登船港口人数按照S、C、Q递减，而且S远多于另外俩港口。<font><br><br>\n",
    "\n",
    "<font color=red>这个时候我们可能会有一些想法了：<font><br><br>\n",
    "\n",
    "1. <font color=red>不同舱位/乘客等级可能和财富/地位有关系，最后获救概率可能会不一样<font><br>\n",
    "2. <font color=red>年龄对获救概率也一定是有影响的，毕竟前面说了，副船长还说『小孩和女士先走』呢<font><br>\n",
    "3. <font color=red>和登船港口是不是有关系呢？也许登船港口不同，人的出身地位不同？<font><br>\n",
    "\n",
    "<font color=red>口说无凭，空想无益。老老实实再来统计统计，看看这些属性值的统计分布吧。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各乘客等级的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_0 = data_train.Pclass[data_train.Survived == 0].value_counts()\n",
    "Survived_1 = data_train.Pclass[data_train.Survived == 1].value_counts()\n",
    "df=pd.DataFrame({u'获救':Survived_1, u'未获救':Survived_0})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"各乘客等级的获救情况\")\n",
    "plt.xlabel(u\"乘客等级\") \n",
    "plt.ylabel(u\"人数\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>得到这个图：<font><br>\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/9.png?imageView/2/w/450/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>啧啧，果然，钱和地位对舱位有影响，进而对获救的可能性也有影响啊←_← <font><br>\n",
    "<font color=red>咳咳，跑题了，我想说的是，明显等级为1的乘客，获救的概率高很多。恩，这个一定是影响最后获救结果的一个特征。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEUCAYAAAAx56EeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEaBJREFUeJzt3X+s3Xddx/Hni7Wzyo9167q59C62QpWhBpgXqEGJMmO2QexIWDKirCGNjdkgEo06jdGaaDISkblIpg1TO2WMZUq66ERmx/zxRyd3MgejktUF2bWTXspWf2CFsrd/3M+Nd93ntrfjfu85vff5SG7O9/v+fs4575ub9NXv53O+35OqQpKkk71o1A1IksaTASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUtWbUDUhnIsl24Oc7hz4B/Fin/lRVXZtkH7Chc/ztwE8DP9o59pvAuQu8333AnwB3roT3rKq/7NS1yhkQOttcAuyuqr+eKyR5CfAh4MGq+pX5g5Pc0za/XlU/eNKx3wLWAa8EfriqTsw79lbg4na8936/C3zbCnpP6XmcYpIkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpywvldDZ6f5Kn5+2fA/wb8M4kP3jS2Lmrir8vyYMnHXs5sxefAexPMv/rFTcA7z/F+/1L215J7yk9R/zKUUlSj1NMkqQuA0KS1GVASJK6zupF6gsvvLA2b9486jYk6azy8MMPf7mqNp5u3FkdEJs3b2ZqamrUbUjSWSXJvy5mnFNMkqQuA0KS1GVASJK6zuo1CElaKl//+teZnp7m+PHjo25lyaxbt46JiQnWrl37gp5vQEgSMD09zUtf+lI2b95MklG3802rKo4ePcr09DRbtmx5Qa/hFJMkAcePH2fDhg0rIhwAkrBhw4Zv6ozIgJCkZqWEw5xv9vcxICRJXa5BnInd5426g2HtPjbqDqSxsfmmv1jS1/vCzW9Z0tdbDgaEJI2B3bt3c+DAAdasmf1n+cSJE2zbtq1bA7r13bt3L2lPBoQkjYm77rqL9evXA/DMM89wyy23dGsLjV1qrkFIkroMCElSlwEhSeoyICRJXS5SS1LH2fix1KXmGYQkqcszCEkaAxdddBHXX389L3rR7P/bn332Wa688spuDViwvpRSVUv+ostlcnKylvUrR72SWlqxDh48yGWXXTbqNpZc7/dK8nBVTZ7uuYNOMSVZn+SeJP+c5GCSH0hyQZL7kzzeHs9vY5Pk1iSHkjya5PIhe5MkndrQaxC/A3y8ql4JvBo4CNwE7K+qrcD+tg9wFbC1/ewCbhu4N0nSKQwWEEleBrwJuB2gqr5WVc8A24G9bdhe4Jq2vR24o2YdANYnuWSo/iRJpzbkGcR3AjPAHyb5dJIPJXkxcHFVPQXQHi9q4zcBT857/nSrPUeSXUmmkkzNzMwM2L4krW5DfoppDXA58J6qeijJ7/D/00k9vW+2eN4KelXtAfbA7CL1UjQqSc+z1B9KOQs/BDJkQEwD01X1UNu/h9mA+FKSS6rqqTaFdGTe+EvnPX8CODxgf5I0Ns7kdt9LfVvvhQwWEFX170meTPLdVfV54Argc+1nB3Bze9zXnnIv8O4kdwFvAI7NTUVJ0mpwJrf7Xg5DXyj3HuDDSc4FngDexey6x91JdgJfBK5tY+8DrgYOAV9tYyVJIzJoQFTVI0DvYowrOmMLuHHIfiRJi+e9mCRJXQaEJKnLm/VJUs9Z+LHUpeYZhCSpyzMISRoDZ3q77+VgQEjSGLjhhhu44YYbuvVRcYpJkpqz+ftxer7Z38eAkCRg3bp1HD16dMWERFVx9OhR1q1b94JfwykmSQImJiaYnp5mJd0let26dUxMTLzg5xsQkgSsXbuWLVu2jLqNseIUkySpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroGDYgkX0jymSSPJJlqtQuS3J/k8fZ4fqsnya1JDiV5NMnlQ/YmSTq15TiD+JGqek1VTbb9m4D9VbUV2N/2Aa4CtrafXcBty9CbJGkBo5hi2g7sbdt7gWvm1e+oWQeA9UkuGUF/kiSGD4gCPpHk4SS7Wu3iqnoKoD1e1OqbgCfnPXe61Z4jya4kU0mmVtI3P0nSuBn6G+XeWFWHk1wE3J/kn08xNp3a874ctqr2AHsAJicnV8aXx0rSGBr0DKKqDrfHI8DHgNcDX5qbOmqPR9rwaeDSeU+fAA4P2Z8kaWGDBUSSFyd56dw28GPAZ4F7gR1t2A5gX9u+F7i+fZppG3BsbipKkrT8hpxiuhj4WJK597mzqj6e5FPA3Ul2Al8Erm3j7wOuBg4BXwXeNWBvkqTTGCwgquoJ4NWd+lHgik69gBuH6keSdGa8klqS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkrqG/E7qFWfz8TtH3cKgvjDqBiSNFc8gJEldBoQkqcuAkCR1GRCSpK7BAyLJOUk+neTP2/6WJA8leTzJR5Oc2+rf0vYPteObh+5NkrSw5TiD+Bng4Lz99wEfqKqtwNPAzlbfCTxdVa8APtDGSZJGZNCASDIBvAX4UNsP8GbgnjZkL3BN297e9mnHr2jjJUkjMPQZxC3ALwDPtv0NwDNVdaLtTwOb2vYm4EmAdvxYG/8cSXYlmUoyNTMzM2TvkrSqDRYQSd4KHKmqh+eXO0NrEcf+v1C1p6omq2py48aNS9CpJKlnyCup3wj8eJKrgXXAy5g9o1ifZE07S5gADrfx08ClwHSSNcB5wFcG7E+SdAqDnUFU1S9V1URVbQauAx6oqp8APgm8vQ3bAexr2/e2fdrxB6rqeWcQkqTlMYrrIH4R+Nkkh5hdY7i91W8HNrT6zwI3jaA3SVKzLDfrq6oHgQfb9hPA6ztjjgPXLkc/kqTT80pqSVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXYu6F1OSXz3NkCNV9XtL0I8kaUws9mZ925i9ZfdCXwG6FzAgJGkFWWxAfKOq/mOhg0n83gZJWmEWuwZxugAwICRphVnsGcTaJC9b4FiAc5aoH0nSmFhsQBwA3rvAsQB/uTTtSJLGxWID4g24SC1Jq4qL1JKkLhepJUldLlJLkrrOdJF6oTWIjy9NO5KkcbGogKiqXx+6EUnSeBnsZn1J1iX5hyT/lOSxJL/e6luSPJTk8SQfTXJuq39L2z/Ujm8eqjdJ0ukNeTfX/wXeXFWvBl4DXJlkG/A+4ANVtRV4GtjZxu8Enq6qVwAfaOMkSSMyWEDUrP9qu2vbTwFvBu5p9b3ANW17e9unHb8iyUJrHpKkgQ36fRBJzknyCHAEuB/4F+CZqjrRhkwDm9r2JuBJgHb8GLBhyP4kSQsbNCCq6htV9RpgAng9cFlvWHvsnS087/qKJLuSTCWZmpmZWbpmJUnPsSzfKFdVzwAPMvu9EuuTzH16agI43LangUsB2vHzgK90XmtPVU1W1eTGjRuHbl2SVq0hP8W0Mcn6tv2twI8CB4FPAm9vw3YA+9r2vW2fdvyBqvIKbUkakcVeKPdCXALsTXIOs0F0d1X9eZLPAXcl+Q3g08DtbfztwB8nOcTsmcN1A/YmSTqNwQKiqh4FXtupP8HsesTJ9ePAtUP1I0k6M8uyBiFJOvsYEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV2DBUSSS5N8MsnBJI8l+ZlWvyDJ/Ukeb4/nt3qS3JrkUJJHk1w+VG+SpNMb8gziBPBzVXUZsA24McmrgJuA/VW1Fdjf9gGuAra2n13AbQP2Jkk6jcECoqqeqqp/bNv/CRwENgHbgb1t2F7gmra9HbijZh0A1ie5ZKj+JEmntixrEEk2A68FHgIurqqnYDZEgIvasE3Ak/OeNt1qkqQRGDwgkrwE+FPgvVX1H6ca2qlV5/V2JZlKMjUzM7NUbUqSTjJoQCRZy2w4fLiq/qyVvzQ3ddQej7T6NHDpvKdPAIdPfs2q2lNVk1U1uXHjxuGal6RVbshPMQW4HThYVb8979C9wI62vQPYN69+ffs00zbg2NxUlCRp+a0Z8LXfCLwT+EySR1rtl4GbgbuT7AS+CFzbjt0HXA0cAr4KvGvA3iRJpzFYQFTV39NfVwC4ojO+gBuH6keSdGa8klqS1GVASJK6DAhJUpcBIUnqMiAkSV1DfsxVGi+7zxt1B8PZfWzUHWgF8gxCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1DVYQCT5gyRHknx2Xu2CJPcnebw9nt/qSXJrkkNJHk1y+VB9SZIWZ8gziD8CrjypdhOwv6q2AvvbPsBVwNb2swu4bcC+JEmLMFhAVNXfAl85qbwd2Nu29wLXzKvfUbMOAOuTXDJUb5Kk01vuNYiLq+opgPZ4UatvAp6cN2661SRJIzIui9Tp1Ko7MNmVZCrJ1MzMzMBtSdLqtdwB8aW5qaP2eKTVp4FL542bAA73XqCq9lTVZFVNbty4cdBmJWk1W+6AuBfY0bZ3APvm1a9vn2baBhybm4qSJI3GmqFeOMlHgB8GLkwyDfwacDNwd5KdwBeBa9vw+4CrgUPAV4F3DdWXJGlxBguIqnrHAoeu6Iwt4MahepEknblxWaSWJI0ZA0KS1DXYFJMkLZnd5426g2HtPjbqDroMCK0am4/fOeoWBvOFUTegFckpJklSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6vFBO0thbyRc5wvhe6OgZhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpK6xCogkVyb5fJJDSW4adT+StJqNTUAkOQf4IHAV8CrgHUleNdquJGn1GpuAAF4PHKqqJ6rqa8BdwPYR9yRJq9Y43e57E/DkvP1p4A0nD0qyC9jVdv8ryeeXobdRuRD48nK9Wd63XO+0Kvi3O7ut9L/fdyxm0DgFRDq1el6hag+wZ/h2Ri/JVFVNjroPnTn/dmc3/36zxmmKaRq4dN7+BHB4RL1I0qo3TgHxKWBrki1JzgWuA+4dcU+StGqNzRRTVZ1I8m7gr4BzgD+oqsdG3NaorYqptBXKv93Zzb8fkKrnTfNLkjRWU0ySpDFiQEiSusZmDUKSRinJtwGvaLufr6r/HWU/48AziDGQ5HVJvn3e/vVJ9iW5NckFo+xNp5fkFUne2Kn/UJKXj6InLV6StUluYfaj9n8I7AWemLsfXJLXjrK/UTIgxsPvA18DSPIm4GbgDuAYfpribHAL8J+d+v+0Yxpv7wdeAnxHVX1/Vb0WuAz4ziS3AX820u5GyE8xjYEk/1RVr27bHwRmqmp323+kql4zyv50akk+W1Xfu8Cxz1TV9y13T1q8JIeArXXSP4btBqJfBq6qqgMjaW7EPIMYD+ckmVsPugJ4YN4x14nG37pTHPvWZetCL9SzJ4cDQFV9g9n/rK3KcAADYlx8BPibJPuYnZb4O5id22Z2mknj7VNJfurkYpKdwMMj6Edn5nNJrj+5mOQngYMj6GdsOMU0JpJsAy4BPlFV/91q3wW8pKr+caTN6ZSSXAx8jNl1pLlAmATOBd5WVf8+qt50ekk2MbvO8D/M/v0KeB2zZ39vq6p/G2F7I2VASEskyY8Ac2sRj1XVA6car/GS5M3A9zB7Z+nHqmr/iFsaOQNCktTlGoQkqcuAkCR1GRCSpC4DQpLU5UVY0iIl2Q1sA0600hrgwAI1zqQ+d+W8NE4MCOnMXFdVzwAkWQ+8d4HaQmNPVZfGilNMkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV1+zFVavCPAHUmebfsvAj6+QI0XUJfGindzlSR1OcUkSeoyICRJXQaEJKnLgJAkdRkQkqSu/wNCx92QskgLgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各登录港口的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_0 = data_train.Embarked[data_train.Survived == 0].value_counts()\n",
    "Survived_1 = data_train.Embarked[data_train.Survived == 1].value_counts()\n",
    "df=pd.DataFrame({u'获救':Survived_1, u'未获救':Survived_0})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"各登录港口乘客的获救情况\")\n",
    "plt.xlabel(u\"登录港口\") \n",
    "plt.ylabel(u\"人数\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/Embarked.png?imageView/2/w/500/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>并没有看出什么...<font><br>\n",
    "\n",
    "<font color=red>那个，看看性别好了<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各性别的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_m = data_train.Survived[data_train.Sex == 'male'].value_counts()\n",
    "Survived_f = data_train.Survived[data_train.Sex == 'female'].value_counts()\n",
    "df=pd.DataFrame({u'男性':Survived_m, u'女性':Survived_f})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"按性别看获救情况\")\n",
    "plt.xlabel(u\"性别\") \n",
    "plt.ylabel(u\"人数\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/10.png?imageView/2/w/450/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>歪果盆友果然很尊重lady，lady first践行得不错。性别无疑也要作为重要特征加入最后的模型之中。<font><br>\n",
    "\n",
    "<font color=red>再来个详细版的好了<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGshJREFUeJzt3X90XOV95/H3B/+Is8GLiZGJg+3gE4mAYalJtax76IYfIYkxWZym0Nh7AiTLoiQLdDlQUjZtiUsOtKRNodlQUgU4gbTBIWlSe1tTh3VMacma2IDxghVihfgIgcGKsZ12jTHG3/3jXsFEHmlmNHdGIz2f1zk6mnnuM89856vRV1fP3PtcRQRmZpaWI8Y6ADMzaz4XfzOzBLn4m5klyMXfzCxBLv5mZgly8TczS5CLv5lZgiaPdQCWBklLgevKbPo+8MEy7Tsi4iJJq4CZZbZfCHwaOLfMtpuAqcM83xrgr4BvTvTnjIiby7SbAS7+1jyzgRUR8b8HGyQdCdwJPBQRv1/aWdJ38puvRcSvD9n2p8A04ETgrIg4WLLtw8Cx+fZyz/cV4N8k8pxmw/K0j5lZglz8zcwS5OJvZpYgF38zswS5+JuZJcjF38wsQS7+ZmYJcvE3M0uQT/KyZvqSpN0l9ycBzwMXS/r1IX0Hz3b9d5IeGrLt3bx5EtM6SaWXo5sJfGmE5/tpfjuV5zQrS7VcxlHS3cCHgZ0RcUqZ7QL+HFgC7AM+ERGPFxSrmZkVpNZpn68Di0fYfh7QkX91AXeMLiwzM2ukmop/RDwMvDxCl6XAvZHZAMyQNLueAM3MrHhFz/kfBzxXcr8/b9sxtKOkLrL/Dpg5c+avHn/88QWHMrFs3ryZhQsXjuqx27dvx/kdnnPbWKPNr3Nb2WOPPRYRMaoDd4ou/irTVvZDhYjoBroBOjs7Y9OmTQWHMrF0dnYy2hzV89gUOLeNNdocObeVSXpltI8t+lDPfmBuyf05wAsFP4eZmdWp6OK/GrhEmUXA3og4bMrHzMzGVk3TPpLuA84CjpHUD3wemAIQEV8lu2LREqCX7FDPTxYZrJmZFaOm4h8RyytsD+CKWsZ87LHHZl111VX09PTU8rAJa9q0acyZM4cpU6bUPdZrr71Gf38/t9xyi/OLc9toRefXdeFNReZ20Jif4Tt58uQ7Tz31VE488USyc8TSFRHs2rWL/v5+5s+fX/d4/f39TJ8+nXe84x2cdNJJBUQ4fjm3jdWI/LouZIrO7aBWWNvnlKlTpyb/AwaQxMyZM9m/f38h4+3fv5+ZM2c6tzi3jdaI/LouZIrO7aCKxV/SNEk/kvSkpKcl/WHePl/So5K2SfqWpKl5+1vy+7359uMrxeAf8JuKzoVz+ybntrGc38ZpRC6q2fN/FTgnIn4FWAgszo/kuQW4NSI6gN3AZXn/y4DdEdEO3Jr3MzOzFlJxzj//EPdf87tT8q8AzgH+c95+D7CCbC2fpfltgO8AX5GkqHIFudc6Pldl6NWZsu3mQscb1/6x4BNmzuwsdrxx7LvPFHtE80ff41VRSn3oC39f6Hhr/+D8Qscbj6qa85c0SdJmYCfwINlysXsi4mDeZXAZByhZ4iHfvpc3l60tHbNL0rYXX3xx7oEDB+p7FU32qU99ikceeWSswxhRd3c3zz//PJs3b2Y85de5baxWz293dzft7e0899xzDc3tT17Yc9hXvVo9t0NVdbRPRLwOLJQ0A/geUO7whsE9+6qWeBhc3uHJJ5/cPmXKlHdVGW/hVqxYwYYNG5g8OUvFwYMHWbRoUdm2FStWAPDoo4/y2c9+lpNPPvmXPn3/+c9/zsqVKzn//POrbt+wYUNDXldXVxc9PT2cdNJJbN26tSHPUYlz21gTMb9dXV1v5LeW5eaLNhFzO1Stx/nvyS84sYhsxc7J+d596TIOg0s89EuaDBzFyCuBjrmVK1cyY8YMAPbs2cNtt91Wtg2gp6eHE044gUmTJnH55Zdz9dVXvzHO4O1a2ycy57axnN/Gmei5reZon7Z8jx9JbwXOBXqA9cCFebdLgVX57dX5ffLtP6h2vn88eOCBB1i8eKRLGthoObeN5fw2znjMbTVz/rOB9ZK2ABuBByPi74DfBa6R1Es2p39X3v8uYGbefg1wffFhj521a9eOux/yeOHcNpbz2zjjMbfVHO2zBTitTPuzwOll2vcDFxUSXYvZt28fe/bs4Z3vfCfbt28f63AmFOe2sZzfxhmvuR3z5R2GauVDM9evX8/ZZ5891mGMXgsfmjnec9vqh2aO9/wOd2hmEUfp1Gu85rYVlncYN8bjvN544dw2lvPbOOM1ty7+NfjhD3/IGWecMdZhTEjObWM5v40zXnPbctM+zTZr1iwuueQSjjgi+zt46NAhFi9eXLbt8ccff+NxU6dOZdWqVTz00ENvtB1xxBE1t09kzm1jOb+Nk0JuNdZHYT755JPPTp48ef7JJ588pnG0iojgxz/+8WHLBI/meqY9PT2ceOKJ9PT0sGDBgiLDHJec28YqOr+HDh2iUl0ocs7/hHfOKGysog2XW0n7IuJtoxmzFf58P3XgwIExPZuvVQyu2z1t2rRCxps2bRq7du1ybnFuG60R+XVdyBSd20FjPu1z8ODB//rUU0+9VPQLG68Gr9hThDlz5tDf38+LL7444f9Nr4Zz21hF5/fhhx+uWPBe2vNKIc8H8PretxY2VtGKzO2gMZ/2Aejs7Ixa/y1MzWj+dS7isSlwbhtrtDmq5nFFrvY5Hlf6HO/TPmZm1mQu/mZmCXLxNzNLkIu/mVmCXPzNzBLk4m9mliAXfzOzBLn4m5klyMXfzCxBLv5mZgly8TczS5CLv5lZglz8zcwS5OJvZpYgF38zswS5+JuZJajm4i9psaRnJPVKur7M9nmS1kt6QtIWSUuKCdXMzIpSsfhLmpsX8x5JTwN/BZwH3Ad8IW/fXFLkfx/YC0zPv+5uUOxmZjZK1ez5HwSujYiTgN8G3gpMA14HHgTujYiFEbEm738UcDpwMnAtMEPSpMIjNzOzUatY/CNiR0Q8nt89Gvg5cFx+f0/J7UE/zcf9KXAX8ATZH4NfIqlL0iZJvgCqmVmT1TrnPwuYCTya3/8AsEzS3ZKOztveB6yLiDnAEuBEYLjLzh8FvKuvr6/GMKyS7u5u2tvbaWtrw/ktlnPbOM5t81Rd/CUdSTbtsy0ifgHcAdwK/BmwA/hS3nUB+R+HiPg/wCTgyKHjRUR3RHRERNu8efPqehF2uK6uLnp7exkYGMD5LZZz2zjObfNUVfwlTQH+BrgTOErSfGA38DFgFfA13pzaGQD+Y/64k4CpwI+LDdvMzOoxuVIHSSKbu++JiD+VtBVYS1bUvxYRT0taC7ycP+R3gJWStuTj7wZ+1JDozcxsVKrZ8z8DuBg4R9Jm4GbgauCfyOb7twCvkv0XQET8r7zP28imfP5LRLzegNjNzGyUKu75R8Q/AyqzaU2ZtsHH3ATcVEdcZmbWQF7ewcwsQS7+ZmYJcvE3M0uQi7+ZWYJc/M3MEuTib2aWIBd/M7MEufibmSXIxd/MLEEu/mZmCXLxNzNLkIu/mVmCXPzNzBLk4m9mliAXfzOzBLn4m5klyMXfzCxBLv5mZgly8TczS5CLv5lZglz8zcwS5OJvZpYgF38zswS5+JuZJcjF38wsQS7+ZmYJqlj8Jc2VtF5Sj6SnJf33vP3tkh6UtC3/fnTeLklfltQraYuk9zb6RZiZWW2q2fM/CFwbEScBi4ArJC0ArgfWRUQHsC6/D3Ae0JF/dQF3FB61mZnVpWLxj4gdEfF4fvtfgB7gOGApcE/e7R7gI/ntpcC9kdkAzJA0u/DIzcxs1Gqa85d0PHAa8ChwbETsgOwPBDAr73Yc8FzJw/rztqFjdeVTRgN9fX21R24j6u7upr29nba2NpzfYjm3jePcNk/VxV/SkcDfAFdHxC9G6lqmLQ5riOiOiI6IaJs3b161YViVurq66O3tZWBgAOe3WM5t4zi3zVNV8Zc0hazw/3VEfDdvfmlwOif/vjNv7wfmljx8DvBCMeGamVkRqjnaR8BdQE9E/FnJptXApfntS4FVJe2X5Ef9LAL2Dk4PmZlZa5hcRZ8zgIuB/ytpMzAdeAtwADgk6TKgD7go778GuAp4FTgEPFRwzGZmVqeKxT8i/pl8Hl/SJOAnwPvIpnc2AssjYmvJQ9qBY8k+EN4taRZmZtZSaj3D93SgNyKejYgDwEqyQztLXQ7cHhG7ASJiJ2Zm1lJqLf7VHMZ5AnCCpEckbZC0uNxA+aGemyRtqjEGMzOrU63Fv5rDOCeTnd17FrAcuFPSjMMelB3q2RkRnTXGYGZmdaq1+FdzGGc/sCoiXouInwHPkP0xMDOzFlFr8d8IdEiaL2kqsIzs0M5SfwucDSDpGLJpoGfrDdTMzIpTU/GPiIPAlcBasjV+7o+IpyXdKOmCvNtaYJekrcB64LqI2FVk0GZmVp9qjvP/JRGxhuxY/tK2G0puB3BN/mVmZi3IF3MxM0uQi7+ZWYJc/M3MEuTib2aWIBd/M7MEufibmSXIxd/MLEEu/mZmCXLxNzNLUM1n+JpZbb77TDFXMf3oe2YXMo4ZeM/fzCxJLv5mZgly8TczS5CLv5lZgsbNB76vdXyukHGmbLu5kHHMzMYz7/mbmSXIxd/MLEEu/mZmCapY/CXdLWmnpKdK2lZIel7S5vxrScm2/yGpV9Izkj7UqMDNzGz0qtnz/zqwuEz7rRGxMP9aAyBpAbAMODl/zF9ImlRUsGZmVoyKxT8iHgZernK8pcDKiHg1In4G9AKn1xGfmZk1QD1z/ldK2pJPCx2dtx0HPFfSpz9vO4ykLknbJA309fXVEYaV093dTXt7O21tbTi/xXJuG8e5bZ7RFv87gHcDC4EdwJfydpXpG+UGiIjuiOiIiLZ58+aNMgwbTldXF729vQwMDOD8Fsu5bRzntnlGVfwj4qWIeD0iDgFf482pnX5gbknXOcAL9YVoZmZFG1Xxl1S6tuxvAINHAq0Glkl6i6T5QAfwo/pCNDOzolVc3kHSfcBZwDGS+oHPA2dJWkg2pbMd+BRARDwt6X5gK3AQuCIiXm9M6GZmNloVi39ELC/TfNcI/W8CbqonKDMzayyf4WtmliAXfzOzBLn4m5klyMXfzCxBLv5mZgly8TczS5CLv5lZglz8zcwSVHPxl7Q4v1BLr6TrR+h3oaSQ1FlfiGZmVrSain9+YZbbgfOABcDy/AIuQ/tNB34beLSIIM3MrFi17vmfDvRGxLMRcQBYSXYBl6G+AHwR2F9nfGZm1gC1Fv+KF2uRdBowNyL+bqSB8ou5bJK0qcYYzMysTrUW/xEv1iLpCOBW4NpKA+UXc+mMCH8mYGbWZLUW/0oXa5kOnAI8JGk7sAhY7Q99zcxaS63FfyPQIWm+pKnAMrILuAAQEXsj4piIOD4ijgc2ABdEhKd2zMxaSE3FPyIOAlcCa4Ee4P78Ai43SrqgEQGamVnxKl7MZaiIWAOsGdJ2wzB9zxpdWGZm1kg+w9fMLEEu/mZmCXLxNzNLkIu/mVmCXPzNzBLk4m9mliAXfzOzBLn4m5klyMXfzCxBFYu/pLsl7ZT0VEnb2yU9KGlb/v3ovF2Svpxf5WuLpPc2MngzMxudavb8vw4sHtJ2PbAuIjqAdfl9yK7w1ZF/dQF3FBOmmZkVqWLxj4iHgZeHNC8F7slv3wN8pKT93shsAGZIml1UsGZmVozRzvkfGxE7APLvs/L2ilf6GpRfyWubpIG+vr5RhmHD6e7upr29nba2NpzfYjm3jePcNk/RH/iOeKWvX2rMruTVERFt8+bNKzgM6+rqore3l4GBAZzfYjm3jePcNs9oi/9Lg9M5+fedeXulK32ZmVkLGG3xXw1cmt++FFhV0n5JftTPImDv4PSQmZm1jooXc5F0H3AWcIykfuDzwB8D90u6DOgDLsq7rwGWAL3APuCTDYjZzMzqVLH4R8TyYTa9v0zfAK6oNygzM2ssn+FrZpYgF38zswS5+JuZJcjF38wsQS7+ZmYJcvE3M0uQi7+ZWYJc/M3MEuTib2aWIBd/M7MEufibmSXIxd/MLEEu/mZmCaq4qqcl4h83FTPOmZ3FjNNq8ZhNMN7zNzNLkIu/mVmCai7+khZLekZSr6Try2y/RtJWSVskrZP0rmJCNTOzotRU/CVNAm4HzgMWAMslLRjS7QmgMyJOBb4DfLGIQM3MrDi17vmfDvRGxLMRcQBYCSwt7RAR6yNiX353AzCn/jDNzKxItRb/44DnSu73523DuQx4oNwGSV2SNkkq6LAOMzOrVq2HeqpMW5TtKH0c6ATOLLc9IrqBboDOzs6yY5hZ8b77zI7Cxvroe2YXNpY1V63Fvx+YW3J/DvDC0E6SzgV+DzgzIl4dfXhmZtYItU77bAQ6JM2XNBVYBqwu7SDpNOAvgQsiYmcxYZqZWZFqKv4RcRC4ElgL9AD3R8TTkm6UdEHe7U+AI4FvS9osafUww5mZ2RipeXmHiFgDrBnSdkPJ7XMLiMvMzBqorrV9JG0H/gV4HTgYEZ2S3g58Czge2A78VkTsri9MMzMrUhHLO5wdEQsjYnAFreuBdRHRAazL75uZWQtpxNo+S4F78tv3AB9pwHOYmVkd6i3+AXxf0mOSuvK2YyNiB0D+fVa5B+YneW2TNNDX11dnGDZUd3c37e3ttLW14fwWy7ltHOe2eeot/mdExHvJ1vq5QtL7qn1gRHRHREdEtM2bN6/OMGyorq4uent7GRgYwPktlnPbOM5t89T1gW9EvJB/3ynpe2Rr/7wkaXZE7JA0G/Cx/maWjA994e8LGWftH5xfyDjDGfWev6S3SZo+eBv4IPAU2Ulfl+bdLgVW1RukmZkVq549/2OB70kaHOebEfEPkjYC90u6DOgDLqo/TDMzK9Koi39EPAv8Spn2XcD76wnKzMway5dxNDNLkIu/mVmCXPzNzBLk4m9mliAXfzOzBLn4m5klyMXfzCxBLv5mZgly8TczS5CLv5lZglz8zcwS5OJvZpYgF38zswS5+JuZJcjF38wsQS7+ZmYJcvE3M0uQi7+ZWYJc/M3MEuTib2aWIBd/M7MEufibmSXIxd/MLEGTxzqA8ey1js8VMs6UbTcXMo6ZWbW8529mliAXfzOzBNVc/CUtlvSMpF5J15fZ/hZJ35L0gqRXJG0v18/MzMZOTcVf0iTgduA8YAGwXNKCId0uA/YArwC/C2wcpp+ZmY2RWvf8Twd6I+LZiDgArASWDumzFNgE9AJ/AZw9TD8zMxsjtR7tcxzwXMn9fuA/lOkj4LmIOChpL9l/AieXdpL0DeAiYFJ+f1+NsZQzGTg4Yg/9UQFPU5NaYxqu/1RJT9bwvMcA7yDL7yRJr9Tw2OFUfi3FqvR8RcUzHnLbarmvRS35rSa3tcZWVX/dUMOI9asYU5XxvLWeAGqhMm1Rpk/FfhFxMXAxgKRNEdFZYyyHB1fQOEWqNaZWfA2Dmh1bpedr5VzVqtVeayvndiL8ThVZ80b72FqnffqBuSX35wAvlOlzCJgraTJwFDCjTD8zMxsjtRb/jUCHpPmSpgLLgNVD+qwGOoEO4DPAQ8P0MzOzMVJT8Y+Ig8CVwFqgB7g/Ip6WdKOkC/JudwFvB6YBXwT+/WC/EYburjnyxo5TpFpjasXXMKjZsVV6vlbOVa1a7bW2cm4nwu/UmNc8RQydsjczs4nOZ/iamSXIxd/MLEFNLf41LA3RK+lRScc3M75mqiIX10jaKmmLpHWS3jUWcTZDpVyU9LtQUkhqqcP2ilRNLiT9Vv7eeFrSN5sdY7NU8TsyT9J6SU/kvydLxiLORpN0t6Sdkp4aZrskfTnP0xZJ761q3GbN+edLQ/wE+ADZ4aAbgeURsbWkz38DTo2IT0taBvxGRHxsmPFWAIt480SJycCGcm0RsaLwF1RjDPnt0vZzgK8B11E+F2cDj0bEPkmfAc4aLhct8FpGnedq3hd5v5uBT5OdQ/IU8P+Gi69RP+961JDf04FvACcCvwY8Duwnf12SOoD7gXMiYrekWRGxs0mxNS23VdaLbuAJ4Fjg/cCvAj8YKf78dkvWiOFikPQ+4F+BeyPilDLblwBXAUvITrr984gYevLtYZq5nv8bS0MASBpc8qH0l3wpsCK//R3gK5IUw/+FWhYRe/LxZgBXD9PWSLXEsCwi9kj6NbIz83ZHxIFyuYiI9SXPsQH4eINfxxvxDYl7xNdSpr1W1bwvIPtj+Zn863fIlg8Zi593PSrl9wPAHcBu4GNkr5W8bfB1XQ7cHhG7Aeot/DXE1uzcVvO+CODf5rdvBP4wIj7cxPduterKbUQ8XGEWZCnZH4YANkiaIWl2ROwYKahmTvuUWxriuOH65IeV7gVmNiW65joOeL7kfrlclLoMeKChEY2diu8LSaeR/ZKvbWJcY2E28IuS++XeFycAJ0h6RNIGSYubFl1zVVMvVpDtFF0DfJts7zdF1eTqMM3c8692aYhKfSaCql+npI+TnTR3ZkMjGjsj5kLSEcCtwPebFtHYqeZ9MZnsBMqzyM6w/ydJpwzuRU4g1eRiOfB1YDrwCPANSYdNiyRgVHWzmXv+1S4NMRegZGmIl5sSXXMN/ctcLhdIOhf4PeCCiHi1SbE1W6X3xXTgFOATwBayudLVwMImxddML/DmNAYM/zuyKiJei4ifAc+Q/TGYaKqpF5eRff4B2WcC08gWhktNNbk6TDOLf7VLQ1ya374Q+MEI8/3j2Ubg3cCM4XKRT3X8JVnhL2petxWN+L6IiL0RcQxwG3Aq2ecfFwCbxyLYBnucbJpzBjCF8r8jf0u2TDqSjiGbBnq2iTE2SzX1oo/sg17I8jANGGheiC1jNXBJftTPImBvpfl+aOK0T7688+DSEJOAuweXhgA2RcRqsqUhviGpl2yPf1mz4mumPBfXAV8F/hPlc/EnwJHAtyUB9EXEBcMOOk5V+b5IxevAGrLVbpcCd+a5uAWYl/dZC3xQ0ta8/3URsWtMom2gKt8X15IdMfdu4DeBT0RE5L8vE4ak+8im+Y6R1A98nmzngIj4Ktl7ZgnZQRD7gE9WM24z5/yJiDVkgZa23VByez/ZGv8peBD4n6WHdw3JxbljEdRYqPS+GNJ+FrxxhMREtC3/uq1kHv+PyI8Gyf8Tvib/mtCqqBdbgTPywylL8zWhRMTyCtsDuKLWcZta/Au2E7hX0qH8/hHAPwzT1ioxNDO2WrX6a2mFn3c9Wjm/zm3rxNY0XtjNzCxBXtvHzCxBLv5mZgly8TczS5CLv5lZglz8zcwS9P8BEXilVq98XZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#然后我们再来看看各种舱级别情况下各性别的获救情况\n",
    "fig=plt.figure()\n",
    "fig.set(alpha=0.65) # 设置图像透明度，无所谓\n",
    "plt.title(u\"根据舱等级和性别的获救情况\")\n",
    "\n",
    "ax1=fig.add_subplot(141)\n",
    "data_train.Survived[data_train.Sex == 'female'][data_train.Pclass != 3].value_counts().plot(kind='bar', label=\"female highclass\", color='#FA2479')\n",
    "ax1.set_xticklabels([u\"获救\", u\"未获救\"], rotation=0)\n",
    "ax1.legend([u\"女性/高级舱\"], loc='best')\n",
    "\n",
    "ax2=fig.add_subplot(142, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'female'][data_train.Pclass == 3].value_counts().plot(kind='bar', label='female, low class', color='pink')\n",
    "ax2.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"女性/低级舱\"], loc='best')\n",
    "\n",
    "ax3=fig.add_subplot(143, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'male'][data_train.Pclass != 3].value_counts().plot(kind='bar', label='male, high class',color='lightblue')\n",
    "ax3.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"男性/高级舱\"], loc='best')\n",
    "\n",
    "ax4=fig.add_subplot(144, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'male'][data_train.Pclass == 3].value_counts().plot(kind='bar', label='male low class', color='steelblue')\n",
    "ax4.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"男性/低级舱\"], loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/11.png?imageView/2/w/700/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>那堂兄弟和父母呢？<font>\n",
    "<font color=red>大家族会有优势么？<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PassengerId\n",
       "SibSp Survived             \n",
       "0     0                 398\n",
       "      1                 210\n",
       "1     0                  97\n",
       "      1                 112\n",
       "2     0                  15\n",
       "      1                  13\n",
       "3     0                  12\n",
       "      1                   4\n",
       "4     0                  15\n",
       "      1                   3\n",
       "5     0                   5\n",
       "8     0                   7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = data_train.groupby(['SibSp','Survived'])\n",
    "df = pd.DataFrame(g.count()['PassengerId'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PassengerId\n",
       "Parch Survived             \n",
       "0     0                 445\n",
       "      1                 233\n",
       "1     0                  53\n",
       "      1                  65\n",
       "2     0                  40\n",
       "      1                  40\n",
       "3     0                   2\n",
       "      1                   3\n",
       "4     0                   4\n",
       "5     0                   4\n",
       "      1                   1\n",
       "6     0                   1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = data_train.groupby(['Parch','Survived'])\n",
    "df = pd.DataFrame(g.count()['PassengerId'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>好吧，没看出特别特别明显的规律(为自己的智商感到捉急…)，先作为备选特征，放一放。<font><br>\n",
    "<font color=red>看看船票好了<font><br>\n",
    "<font color=red>ticket是船票编号，应该是unique的，和最后的结果没有太大的关系，不纳入考虑的特征范畴<font><br>\n",
    "<font color=red>cabin只有204个乘客有值，我们先看看它的一个分布<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G6             4\n",
       "B96 B98        4\n",
       "C23 C25 C27    4\n",
       "C22 C26        3\n",
       "F2             3\n",
       "F33            3\n",
       "E101           3\n",
       "D              3\n",
       "C93            2\n",
       "B77            2\n",
       "C92            2\n",
       "D17            2\n",
       "C123           2\n",
       "B49            2\n",
       "B20            2\n",
       "D36            2\n",
       "E44            2\n",
       "D26            2\n",
       "E33            2\n",
       "B22            2\n",
       "C126           2\n",
       "E67            2\n",
       "C124           2\n",
       "C68            2\n",
       "C78            2\n",
       "C125           2\n",
       "C65            2\n",
       "D33            2\n",
       "B18            2\n",
       "E121           2\n",
       "              ..\n",
       "C104           1\n",
       "C111           1\n",
       "A36            1\n",
       "B79            1\n",
       "C32            1\n",
       "A6             1\n",
       "C86            1\n",
       "D6             1\n",
       "B3             1\n",
       "D9             1\n",
       "C54            1\n",
       "B102           1\n",
       "A16            1\n",
       "D45            1\n",
       "E77            1\n",
       "B50            1\n",
       "C110           1\n",
       "C90            1\n",
       "E46            1\n",
       "C7             1\n",
       "E49            1\n",
       "E17            1\n",
       "D11            1\n",
       "C87            1\n",
       "A31            1\n",
       "A24            1\n",
       "D48            1\n",
       "C103           1\n",
       "D15            1\n",
       "B82 B84        1\n",
       "Name: Cabin, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ticket是船票编号，应该是unique的，和最后的结果没有太大的关系，不纳入考虑的特征范畴\n",
    "#cabin只有204个乘客有值，我们先看看它的一个分布\n",
    "data_train.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>这三三两两的…如此不集中…我们猜一下，也许，前面的ABCDE是指的甲板位置、然后编号是房间号？…好吧，我瞎说的，别当真…<font><br>\n",
    "<font color=red>关键是Cabin这鬼属性，应该算作类目型的，本来缺失值就多，还如此不集中，注定是个棘手货…第一感觉，这玩意儿如果直接按照类目特征处理的话，太散了，估计每个因子化后的特征都拿不到什么权重。加上有那么多缺失值，要不我们先把Cabin缺失与否作为条件(虽然这部分信息缺失可能并非未登记，maybe只是丢失了而已，所以这样做未必妥当)，先在有无Cabin信息这个粗粒度上看看Survived的情况好了。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cabin的值计数太分散了，绝大多数Cabin值只出现一次。感觉上作为类目，加入特征未必会有效\n",
    "#那我们一起看看这个值的有无，对于survival的分布状况，影响如何吧\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_cabin = data_train.Survived[pd.notnull(data_train.Cabin)].value_counts()\n",
    "Survived_nocabin = data_train.Survived[pd.isnull(data_train.Cabin)].value_counts()\n",
    "df=pd.DataFrame({u'有':Survived_cabin, u'无':Survived_nocabin}).transpose()\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"按Cabin有无看获救情况\")\n",
    "plt.xlabel(u\"Cabin有无\") \n",
    "plt.ylabel(u\"人数\")\n",
    "plt.show()\n",
    "\n",
    "#似乎有cabin记录的乘客survival比例稍高，那先试试把这个值分为两类，有cabin值/无cabin值，一会儿加到类别特征好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/15.png?imageView/2/w/400/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>有Cabin记录的似乎获救概率稍高一些，先这么着放一放吧。<font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>先从最突出的数据属性开始吧，对，Cabin和Age，有丢失数据实在是对下一步工作影响太大。<font><br>\n",
    "\n",
    "<font color=red>先说Cabin，暂时我们就按照刚才说的，按Cabin有无数据，将这个属性处理成Yes和No两种类型吧。<font><br>\n",
    "\n",
    "<font color=red>再说Age：<font><br>\n",
    "\n",
    "<font color=red>通常遇到缺值的情况，我们会有几种常见的处理方式<font><br>\n",
    "\n",
    "1. <font color=red>如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了<font><br>\n",
    "2. <font color=red>如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中<font><br>\n",
    "3. <font color=red>如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。<font><br>\n",
    "4. <font color=red>有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上。<font><br>\n",
    "<font color=red>本例中，后两种处理方式应该都是可行的，我们先试试拟合补全吧(虽然说没有特别多的背景可供我们拟合，这不一定是一个多么好的选择)<font><br>\n",
    "\n",
    "<font color=red>我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>23.838953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>32.066493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>22.380113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Giles, Mr. Frederick Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28134</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Swift, Mrs. Frederick Joel (Margaret Welles Ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17466</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>10.869867</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Gill, Mr. John William</td>\n",
       "      <td>male</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233866</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236852</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Duran y More, Miss. Asuncion</td>\n",
       "      <td>female</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2149</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roebling, Mr. Washington Augustus II</td>\n",
       "      <td>male</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17590</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>25.977889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345777</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Master. Harold Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Balkic, Mr. Cerin</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349248</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Cruyssen, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345765</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Abelson, Mrs. Samuel (Hannah Wizosky)</td>\n",
       "      <td>female</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P/PP 3381</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Najib, Miss. Adele Kiamie \"Jane\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2667</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Alfred Ossian</td>\n",
       "      <td>male</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7534</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Petroff, Mr. Nedelio</td>\n",
       "      <td>male</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349212</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Laleff, Mr. Kristo</td>\n",
       "      <td>male</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349217</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)</td>\n",
       "      <td>female</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Shelley, Mrs. William (Imanita Parrish Hall)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230433</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Markun, Mr. Johann</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349257</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dahlberg, Miss. Gerda Ulrika</td>\n",
       "      <td>female</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7552</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Banfield, Mr. Frederick James</td>\n",
       "      <td>male</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A./SOTON 34068</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sutehall, Mr. Henry Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 392076</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Mrs. William (Margaret Norton)</td>\n",
       "      <td>female</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>16.193950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "5              6         0       3   \n",
       "6              7         0       1   \n",
       "7              8         0       3   \n",
       "8              9         1       3   \n",
       "9             10         1       2   \n",
       "10            11         1       3   \n",
       "11            12         1       1   \n",
       "12            13         0       3   \n",
       "13            14         0       3   \n",
       "14            15         0       3   \n",
       "15            16         1       2   \n",
       "16            17         0       3   \n",
       "17            18         1       2   \n",
       "18            19         0       3   \n",
       "19            20         1       3   \n",
       "20            21         0       2   \n",
       "21            22         1       2   \n",
       "22            23         1       3   \n",
       "23            24         1       1   \n",
       "24            25         0       3   \n",
       "25            26         1       3   \n",
       "26            27         0       3   \n",
       "27            28         0       1   \n",
       "28            29         1       3   \n",
       "29            30         0       3   \n",
       "..           ...       ...     ...   \n",
       "861          862         0       2   \n",
       "862          863         1       1   \n",
       "863          864         0       3   \n",
       "864          865         0       2   \n",
       "865          866         1       2   \n",
       "866          867         1       2   \n",
       "867          868         0       1   \n",
       "868          869         0       3   \n",
       "869          870         1       3   \n",
       "870          871         0       3   \n",
       "871          872         1       1   \n",
       "872          873         0       1   \n",
       "873          874         0       3   \n",
       "874          875         1       2   \n",
       "875          876         1       3   \n",
       "876          877         0       3   \n",
       "877          878         0       3   \n",
       "878          879         0       3   \n",
       "879          880         1       1   \n",
       "880          881         1       2   \n",
       "881          882         0       3   \n",
       "882          883         0       3   \n",
       "883          884         0       2   \n",
       "884          885         0       3   \n",
       "885          886         0       3   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex        Age  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.000000   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.000000   \n",
       "2                               Heikkinen, Miss. Laina  female  26.000000   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
       "4                             Allen, Mr. William Henry    male  35.000000   \n",
       "5                                     Moran, Mr. James    male  23.838953   \n",
       "6                              McCarthy, Mr. Timothy J    male  54.000000   \n",
       "7                       Palsson, Master. Gosta Leonard    male   2.000000   \n",
       "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.000000   \n",
       "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.000000   \n",
       "10                     Sandstrom, Miss. Marguerite Rut  female   4.000000   \n",
       "11                            Bonnell, Miss. Elizabeth  female  58.000000   \n",
       "12                      Saundercock, Mr. William Henry    male  20.000000   \n",
       "13                         Andersson, Mr. Anders Johan    male  39.000000   \n",
       "14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.000000   \n",
       "15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.000000   \n",
       "16                                Rice, Master. Eugene    male   2.000000   \n",
       "17                        Williams, Mr. Charles Eugene    male  32.066493   \n",
       "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.000000   \n",
       "19                             Masselmani, Mrs. Fatima  female  29.518205   \n",
       "20                                Fynney, Mr. Joseph J    male  35.000000   \n",
       "21                               Beesley, Mr. Lawrence    male  34.000000   \n",
       "22                         McGowan, Miss. Anna \"Annie\"  female  15.000000   \n",
       "23                        Sloper, Mr. William Thompson    male  28.000000   \n",
       "24                       Palsson, Miss. Torborg Danira  female   8.000000   \n",
       "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.000000   \n",
       "26                             Emir, Mr. Farred Chehab    male  29.518205   \n",
       "27                      Fortune, Mr. Charles Alexander    male  19.000000   \n",
       "28                       O'Dwyer, Miss. Ellen \"Nellie\"  female  22.380113   \n",
       "29                                 Todoroff, Mr. Lalio    male  27.947206   \n",
       "..                                                 ...     ...        ...   \n",
       "861                        Giles, Mr. Frederick Edward    male  21.000000   \n",
       "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.000000   \n",
       "863                  Sage, Miss. Dorothy Edith \"Dolly\"  female  10.869867   \n",
       "864                             Gill, Mr. John William    male  24.000000   \n",
       "865                           Bystrom, Mrs. (Karolina)  female  42.000000   \n",
       "866                       Duran y More, Miss. Asuncion  female  27.000000   \n",
       "867               Roebling, Mr. Washington Augustus II    male  31.000000   \n",
       "868                        van Melkebeke, Mr. Philemon    male  25.977889   \n",
       "869                    Johnson, Master. Harold Theodor    male   4.000000   \n",
       "870                                  Balkic, Mr. Cerin    male  26.000000   \n",
       "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.000000   \n",
       "872                           Carlsson, Mr. Frans Olof    male  33.000000   \n",
       "873                        Vander Cruyssen, Mr. Victor    male  47.000000   \n",
       "874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.000000   \n",
       "875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.000000   \n",
       "876                      Gustafsson, Mr. Alfred Ossian    male  20.000000   \n",
       "877                               Petroff, Mr. Nedelio    male  19.000000   \n",
       "878                                 Laleff, Mr. Kristo    male  27.947206   \n",
       "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.000000   \n",
       "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.000000   \n",
       "881                                 Markun, Mr. Johann    male  33.000000   \n",
       "882                       Dahlberg, Miss. Gerda Ulrika  female  22.000000   \n",
       "883                      Banfield, Mr. Frederick James    male  28.000000   \n",
       "884                             Sutehall, Mr. Henry Jr    male  25.000000   \n",
       "885               Rice, Mrs. William (Margaret Norton)  female  39.000000   \n",
       "886                              Montvila, Rev. Juozas    male  27.000000   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.000000   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  16.193950   \n",
       "889                              Behr, Mr. Karl Howell    male  26.000000   \n",
       "890                                Dooley, Mr. Patrick    male  32.000000   \n",
       "\n",
       "     SibSp  Parch            Ticket      Fare Cabin Embarked  \n",
       "0        1      0         A/5 21171    7.2500    No        S  \n",
       "1        1      0          PC 17599   71.2833   Yes        C  \n",
       "2        0      0  STON/O2. 3101282    7.9250    No        S  \n",
       "3        1      0            113803   53.1000   Yes        S  \n",
       "4        0      0            373450    8.0500    No        S  \n",
       "5        0      0            330877    8.4583    No        Q  \n",
       "6        0      0             17463   51.8625   Yes        S  \n",
       "7        3      1            349909   21.0750    No        S  \n",
       "8        0      2            347742   11.1333    No        S  \n",
       "9        1      0            237736   30.0708    No        C  \n",
       "10       1      1           PP 9549   16.7000   Yes        S  \n",
       "11       0      0            113783   26.5500   Yes        S  \n",
       "12       0      0         A/5. 2151    8.0500    No        S  \n",
       "13       1      5            347082   31.2750    No        S  \n",
       "14       0      0            350406    7.8542    No        S  \n",
       "15       0      0            248706   16.0000    No        S  \n",
       "16       4      1            382652   29.1250    No        Q  \n",
       "17       0      0            244373   13.0000    No        S  \n",
       "18       1      0            345763   18.0000    No        S  \n",
       "19       0      0              2649    7.2250    No        C  \n",
       "20       0      0            239865   26.0000    No        S  \n",
       "21       0      0            248698   13.0000   Yes        S  \n",
       "22       0      0            330923    8.0292    No        Q  \n",
       "23       0      0            113788   35.5000   Yes        S  \n",
       "24       3      1            349909   21.0750    No        S  \n",
       "25       1      5            347077   31.3875    No        S  \n",
       "26       0      0              2631    7.2250    No        C  \n",
       "27       3      2             19950  263.0000   Yes        S  \n",
       "28       0      0            330959    7.8792    No        Q  \n",
       "29       0      0            349216    7.8958    No        S  \n",
       "..     ...    ...               ...       ...   ...      ...  \n",
       "861      1      0             28134   11.5000    No        S  \n",
       "862      0      0             17466   25.9292   Yes        S  \n",
       "863      8      2          CA. 2343   69.5500    No        S  \n",
       "864      0      0            233866   13.0000    No        S  \n",
       "865      0      0            236852   13.0000    No        S  \n",
       "866      1      0     SC/PARIS 2149   13.8583    No        C  \n",
       "867      0      0          PC 17590   50.4958   Yes        S  \n",
       "868      0      0            345777    9.5000    No        S  \n",
       "869      1      1            347742   11.1333    No        S  \n",
       "870      0      0            349248    7.8958    No        S  \n",
       "871      1      1             11751   52.5542   Yes        S  \n",
       "872      0      0               695    5.0000   Yes        S  \n",
       "873      0      0            345765    9.0000    No        S  \n",
       "874      1      0         P/PP 3381   24.0000    No        C  \n",
       "875      0      0              2667    7.2250    No        C  \n",
       "876      0      0              7534    9.8458    No        S  \n",
       "877      0      0            349212    7.8958    No        S  \n",
       "878      0      0            349217    7.8958    No        S  \n",
       "879      0      1             11767   83.1583   Yes        C  \n",
       "880      0      1            230433   26.0000    No        S  \n",
       "881      0      0            349257    7.8958    No        S  \n",
       "882      0      0              7552   10.5167    No        S  \n",
       "883      0      0  C.A./SOTON 34068   10.5000    No        S  \n",
       "884      0      0   SOTON/OQ 392076    7.0500    No        S  \n",
       "885      0      5            382652   29.1250    No        Q  \n",
       "886      0      0            211536   13.0000    No        S  \n",
       "887      0      0            112053   30.0000   Yes        S  \n",
       "888      1      2        W./C. 6607   23.4500    No        S  \n",
       "889      0      0            111369   30.0000   Yes        C  \n",
       "890      0      0            370376    7.7500    No        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]\n",
    "\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges\n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "data_train, rfr = set_missing_ages(data_train)\n",
    "data_train = set_Cabin_type(data_train)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>因为逻辑回归建模时，需要输入的特征都是数值型特征，我们通常会先对类目型的特征因子化/one-hot编码。 <font><br>\n",
    "<font color=red>什么叫做因子化/one-hot编码？举个例子：<font><br>\n",
    "\n",
    "<font color=red>以Embarked为例，原本一个属性维度，因为其取值可以是[‘S’,’C’,’Q‘]，而将其平展开为’Embarked_C’,’Embarked_S’, ‘Embarked_Q’三个属性<font><br>\n",
    "\n",
    "* <font color=red>原本Embarked取值为S的，在此处的”Embarked_S”下取值为1，在’Embarked_C’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=red>原本Embarked取值为C的，在此处的”Embarked_C”下取值为1，在’Embarked_S’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=red>原本Embarked取值为Q的，在此处的”Embarked_Q”下取值为1，在’Embarked_C’, ‘Embarked_S’下取值为0<font><br>\n",
    "\n",
    "<font color=red>我们使用pandas的”get_dummies”来完成这个工作，并拼接在原来的”data_train”之上，如下所示。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin_No</th>\n",
       "      <th>Cabin_Yes</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23.838953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>32.066493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>22.380113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>10.869867</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>25.977889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>16.193950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived        Age  SibSp  Parch      Fare  Cabin_No  \\\n",
       "0              1         0  22.000000      1      0    7.2500         1   \n",
       "1              2         1  38.000000      1      0   71.2833         0   \n",
       "2              3         1  26.000000      0      0    7.9250         1   \n",
       "3              4         1  35.000000      1      0   53.1000         0   \n",
       "4              5         0  35.000000      0      0    8.0500         1   \n",
       "5              6         0  23.838953      0      0    8.4583         1   \n",
       "6              7         0  54.000000      0      0   51.8625         0   \n",
       "7              8         0   2.000000      3      1   21.0750         1   \n",
       "8              9         1  27.000000      0      2   11.1333         1   \n",
       "9             10         1  14.000000      1      0   30.0708         1   \n",
       "10            11         1   4.000000      1      1   16.7000         0   \n",
       "11            12         1  58.000000      0      0   26.5500         0   \n",
       "12            13         0  20.000000      0      0    8.0500         1   \n",
       "13            14         0  39.000000      1      5   31.2750         1   \n",
       "14            15         0  14.000000      0      0    7.8542         1   \n",
       "15            16         1  55.000000      0      0   16.0000         1   \n",
       "16            17         0   2.000000      4      1   29.1250         1   \n",
       "17            18         1  32.066493      0      0   13.0000         1   \n",
       "18            19         0  31.000000      1      0   18.0000         1   \n",
       "19            20         1  29.518205      0      0    7.2250         1   \n",
       "20            21         0  35.000000      0      0   26.0000         1   \n",
       "21            22         1  34.000000      0      0   13.0000         0   \n",
       "22            23         1  15.000000      0      0    8.0292         1   \n",
       "23            24         1  28.000000      0      0   35.5000         0   \n",
       "24            25         0   8.000000      3      1   21.0750         1   \n",
       "25            26         1  38.000000      1      5   31.3875         1   \n",
       "26            27         0  29.518205      0      0    7.2250         1   \n",
       "27            28         0  19.000000      3      2  263.0000         0   \n",
       "28            29         1  22.380113      0      0    7.8792         1   \n",
       "29            30         0  27.947206      0      0    7.8958         1   \n",
       "..           ...       ...        ...    ...    ...       ...       ...   \n",
       "861          862         0  21.000000      1      0   11.5000         1   \n",
       "862          863         1  48.000000      0      0   25.9292         0   \n",
       "863          864         0  10.869867      8      2   69.5500         1   \n",
       "864          865         0  24.000000      0      0   13.0000         1   \n",
       "865          866         1  42.000000      0      0   13.0000         1   \n",
       "866          867         1  27.000000      1      0   13.8583         1   \n",
       "867          868         0  31.000000      0      0   50.4958         0   \n",
       "868          869         0  25.977889      0      0    9.5000         1   \n",
       "869          870         1   4.000000      1      1   11.1333         1   \n",
       "870          871         0  26.000000      0      0    7.8958         1   \n",
       "871          872         1  47.000000      1      1   52.5542         0   \n",
       "872          873         0  33.000000      0      0    5.0000         0   \n",
       "873          874         0  47.000000      0      0    9.0000         1   \n",
       "874          875         1  28.000000      1      0   24.0000         1   \n",
       "875          876         1  15.000000      0      0    7.2250         1   \n",
       "876          877         0  20.000000      0      0    9.8458         1   \n",
       "877          878         0  19.000000      0      0    7.8958         1   \n",
       "878          879         0  27.947206      0      0    7.8958         1   \n",
       "879          880         1  56.000000      0      1   83.1583         0   \n",
       "880          881         1  25.000000      0      1   26.0000         1   \n",
       "881          882         0  33.000000      0      0    7.8958         1   \n",
       "882          883         0  22.000000      0      0   10.5167         1   \n",
       "883          884         0  28.000000      0      0   10.5000         1   \n",
       "884          885         0  25.000000      0      0    7.0500         1   \n",
       "885          886         0  39.000000      0      5   29.1250         1   \n",
       "886          887         0  27.000000      0      0   13.0000         1   \n",
       "887          888         1  19.000000      0      0   30.0000         0   \n",
       "888          889         0  16.193950      1      2   23.4500         1   \n",
       "889          890         1  26.000000      0      0   30.0000         0   \n",
       "890          891         0  32.000000      0      0    7.7500         1   \n",
       "\n",
       "     Cabin_Yes  Embarked_C  Embarked_Q  Embarked_S  Sex_female  Sex_male  \\\n",
       "0            0           0           0           1           0         1   \n",
       "1            1           1           0           0           1         0   \n",
       "2            0           0           0           1           1         0   \n",
       "3            1           0           0           1           1         0   \n",
       "4            0           0           0           1           0         1   \n",
       "5            0           0           1           0           0         1   \n",
       "6            1           0           0           1           0         1   \n",
       "7            0           0           0           1           0         1   \n",
       "8            0           0           0           1           1         0   \n",
       "9            0           1           0           0           1         0   \n",
       "10           1           0           0           1           1         0   \n",
       "11           1           0           0           1           1         0   \n",
       "12           0           0           0           1           0         1   \n",
       "13           0           0           0           1           0         1   \n",
       "14           0           0           0           1           1         0   \n",
       "15           0           0           0           1           1         0   \n",
       "16           0           0           1           0           0         1   \n",
       "17           0           0           0           1           0         1   \n",
       "18           0           0           0           1           1         0   \n",
       "19           0           1           0           0           1         0   \n",
       "20           0           0           0           1           0         1   \n",
       "21           1           0           0           1           0         1   \n",
       "22           0           0           1           0           1         0   \n",
       "23           1           0           0           1           0         1   \n",
       "24           0           0           0           1           1         0   \n",
       "25           0           0           0           1           1         0   \n",
       "26           0           1           0           0           0         1   \n",
       "27           1           0           0           1           0         1   \n",
       "28           0           0           1           0           1         0   \n",
       "29           0           0           0           1           0         1   \n",
       "..         ...         ...         ...         ...         ...       ...   \n",
       "861          0           0           0           1           0         1   \n",
       "862          1           0           0           1           1         0   \n",
       "863          0           0           0           1           1         0   \n",
       "864          0           0           0           1           0         1   \n",
       "865          0           0           0           1           1         0   \n",
       "866          0           1           0           0           1         0   \n",
       "867          1           0           0           1           0         1   \n",
       "868          0           0           0           1           0         1   \n",
       "869          0           0           0           1           0         1   \n",
       "870          0           0           0           1           0         1   \n",
       "871          1           0           0           1           1         0   \n",
       "872          1           0           0           1           0         1   \n",
       "873          0           0           0           1           0         1   \n",
       "874          0           1           0           0           1         0   \n",
       "875          0           1           0           0           1         0   \n",
       "876          0           0           0           1           0         1   \n",
       "877          0           0           0           1           0         1   \n",
       "878          0           0           0           1           0         1   \n",
       "879          1           1           0           0           1         0   \n",
       "880          0           0           0           1           1         0   \n",
       "881          0           0           0           1           0         1   \n",
       "882          0           0           0           1           1         0   \n",
       "883          0           0           0           1           0         1   \n",
       "884          0           0           0           1           0         1   \n",
       "885          0           0           1           0           1         0   \n",
       "886          0           0           0           1           0         1   \n",
       "887          1           0           0           1           1         0   \n",
       "888          0           0           0           1           1         0   \n",
       "889          1           1           0           0           0         1   \n",
       "890          0           0           1           0           0         1   \n",
       "\n",
       "     Pclass_1  Pclass_2  Pclass_3  \n",
       "0           0         0         1  \n",
       "1           1         0         0  \n",
       "2           0         0         1  \n",
       "3           1         0         0  \n",
       "4           0         0         1  \n",
       "5           0         0         1  \n",
       "6           1         0         0  \n",
       "7           0         0         1  \n",
       "8           0         0         1  \n",
       "9           0         1         0  \n",
       "10          0         0         1  \n",
       "11          1         0         0  \n",
       "12          0         0         1  \n",
       "13          0         0         1  \n",
       "14          0         0         1  \n",
       "15          0         1         0  \n",
       "16          0         0         1  \n",
       "17          0         1         0  \n",
       "18          0         0         1  \n",
       "19          0         0         1  \n",
       "20          0         1         0  \n",
       "21          0         1         0  \n",
       "22          0         0         1  \n",
       "23          1         0         0  \n",
       "24          0         0         1  \n",
       "25          0         0         1  \n",
       "26          0         0         1  \n",
       "27          1         0         0  \n",
       "28          0         0         1  \n",
       "29          0         0         1  \n",
       "..        ...       ...       ...  \n",
       "861         0         1         0  \n",
       "862         1         0         0  \n",
       "863         0         0         1  \n",
       "864         0         1         0  \n",
       "865         0         1         0  \n",
       "866         0         1         0  \n",
       "867         1         0         0  \n",
       "868         0         0         1  \n",
       "869         0         0         1  \n",
       "870         0         0         1  \n",
       "871         1         0         0  \n",
       "872         1         0         0  \n",
       "873         0         0         1  \n",
       "874         0         1         0  \n",
       "875         0         0         1  \n",
       "876         0         0         1  \n",
       "877         0         0         1  \n",
       "878         0         0         1  \n",
       "879         1         0         0  \n",
       "880         0         1         0  \n",
       "881         0         0         1  \n",
       "882         0         0         1  \n",
       "883         0         1         0  \n",
       "884         0         0         1  \n",
       "885         0         0         1  \n",
       "886         0         1         0  \n",
       "887         1         0         0  \n",
       "888         0         0         1  \n",
       "889         1         0         0  \n",
       "890         0         0         1  \n",
       "\n",
       "[891 rows x 16 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为逻辑回归建模时，需要输入的特征都是数值型特征\n",
    "# 我们先对类目型的特征离散/因子化\n",
    "# 以Cabin为例，原本一个属性维度，因为其取值可以是['yes','no']，而将其平展开为'Cabin_yes','Cabin_no'两个属性\n",
    "# 原本Cabin取值为yes的，在此处的'Cabin_yes'下取值为1，在'Cabin_no'下取值为0\n",
    "# 原本Cabin取值为no的，在此处的'Cabin_yes'下取值为0，在'Cabin_no'下取值为1\n",
    "# 我们使用pandas的get_dummies来完成这个工作，并拼接在原来的data_train之上，如下所示\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>我们还得做一些处理，仔细看看Age和Fare两个属性，乘客的数值幅度变化，也忒大了吧！！如果大家了解逻辑回归与梯度下降的话，会知道，各属性值之间scale差距太大，将对收敛速度造成几万点伤害值！甚至不收敛！ (╬▔皿▔)…所以我们先用scikit-learn里面的preprocessing模块对这俩货做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[22.         38.         26.         35.         35.         23.83895259\n 54.          2.         27.         14.          4.         58.\n 20.         39.         14.         55.          2.         32.06649305\n 31.         29.51820514 35.         34.         15.         28.\n  8.         38.         29.51820514 19.         22.38011324 27.94720616\n 40.         36.10804822 35.2958243  66.         28.         42.\n 22.87630686 21.         18.         14.         40.         27.\n 27.94720616  3.         19.         30.70572678 33.12898535 35.2958243\n 23.45968333 18.          7.         21.         49.         29.\n 65.         44.06483036 21.         28.5         5.         11.\n 22.         38.         45.          4.         41.20008848 17.09991595\n 29.         19.         17.         26.         32.         16.\n 21.         26.         32.         25.         27.94720616 30.70572678\n  0.83       30.         22.         29.         23.32262739 28.\n 17.         33.         16.         30.70572678 23.         24.\n 29.         20.         46.         26.         59.         30.70572678\n 71.         23.         34.         34.         28.         27.94720616\n 21.         33.         37.         28.         21.         27.51545426\n 38.         33.55117591 47.         14.5        22.         20.\n 17.         21.         70.5        29.         24.          2.\n 21.         30.70572678 32.5        32.5        54.         12.\n 35.2958243  24.         25.78337698 45.         33.         20.\n 47.         29.         25.         23.         19.         37.\n 16.         24.         25.34409583 22.         24.         19.\n 18.         19.         27.          9.         36.5        42.\n 51.         22.         55.5        40.5        29.78279613 51.\n 16.         30.         25.52340334 10.86986696 44.         40.\n 26.         17.          1.          9.         26.03188214 45.\n 49.5542756  28.         61.          4.          1.         21.\n 56.         18.          7.30954704 50.         30.         36.\n 10.86986696 31.71894048  9.          1.          4.         46.24976824\n 33.12898535 45.         40.         36.         32.         19.\n 19.          3.         44.         58.         35.2958243  42.\n 35.2958243  24.         28.         10.86986696 34.         45.5\n 18.          2.         32.         26.         16.         40.\n 24.         35.         22.         30.         31.09342985 31.\n 27.         42.         32.         30.         16.         27.\n 51.         27.94720616 38.         22.         19.         20.5\n 18.          7.30954704 35.         29.         59.          5.\n 24.         31.10838452 44.          8.         19.         33.\n 20.80015413 33.12898535 29.         22.         30.         44.\n 25.         24.         37.         54.         29.78279613 29.\n 62.         30.         41.         29.         34.62028571 30.\n 35.         50.         35.2958243   3.         52.         40.\n 35.2958243  36.         16.         25.         58.         35.\n 36.87489821 25.         41.         37.         35.2958243  63.\n 45.         35.05181757  7.         35.         65.         28.\n 16.         19.         57.74249226 33.         30.         22.\n 42.         22.         26.         19.         36.         24.\n 24.         41.20008848 23.5         2.         41.57487718 50.\n 35.2958243  23.31368333 19.         42.57451554 30.70572678  0.92\n 28.57888393 17.         30.         30.         24.         18.\n 26.         28.         43.         26.         24.         54.\n 31.         40.         22.         27.         30.         22.\n 10.86986696 36.         61.         36.         31.         16.\n 23.31368333 45.5        38.         16.         31.42587794 27.94720616\n 29.         41.         45.         45.          2.         24.\n 28.         25.         36.         24.         40.         26.68916849\n  3.         42.         23.         59.96916448 15.         25.\n 29.51820514 28.         22.         38.         22.38011324 22.38011324\n 40.         29.         45.         35.         33.12898535 30.\n 60.         22.87630686 35.2958243  24.         25.         18.\n 19.         22.          3.         31.94479345 22.         27.\n 20.         19.         42.          1.         32.         35.\n 27.94720616 18.          1.         36.         19.89558113 17.\n 36.         21.         28.         23.         24.         22.\n 31.         46.         23.         28.         39.         26.\n 21.         28.         20.         34.         51.          3.\n 21.          7.30954704 27.94720616 20.68131488 33.         35.05181757\n 44.         30.70572678 34.         18.         30.         10.\n 27.94720616 21.         29.         28.         18.         29.78279613\n 28.         19.         35.2958243  32.         28.         26.68916849\n 42.         17.         50.         14.         21.         24.\n 64.         31.         45.         20.         25.         28.\n 23.47643239  4.         13.         34.          5.         52.\n 36.         24.10899881 30.         49.         30.70572678 29.\n 65.         44.06001827 50.         35.2958243  48.         34.\n 47.         48.         30.70572678 38.         35.05181757 56.\n 19.89558113  0.75       29.78279613 38.         33.         23.\n 22.         44.65953056 34.         29.         22.          2.\n  9.         35.05181757 50.         63.         25.          7.30954704\n 35.         58.         30.          9.         24.10899881 21.\n 55.         71.         21.         18.19479484 54.         25.7554753\n 25.         24.         17.         21.         26.82073281 37.\n 16.         18.         33.         49.8994093  28.         26.\n 29.         30.70572678 36.         54.         24.         47.\n 34.         34.17374861 36.         32.         30.         22.\n 29.51820514 44.         22.87630686 40.5        50.         38.5155\n 39.         23.          2.         22.87630686 17.          8.01370298\n 30.          7.         45.         30.         24.33021696 22.\n 36.          9.         11.         32.         50.         64.\n 19.         32.79502428 33.          8.         17.         27.\n 26.3814249  22.         22.         62.         48.         38.5155\n 39.         36.         35.2958243  40.         28.         30.70572678\n 30.70572678 24.         19.         29.         22.87630686 32.\n 62.         53.         36.         35.2958243  16.         19.\n 34.         39.         20.07016773 32.         25.         39.\n 54.         36.         29.04293865 18.         47.         60.\n 22.         30.70572678 35.         52.         47.         27.45639428\n 37.         36.         24.40235514 49.         29.51820514 49.\n 24.         27.94720616 37.51615833 44.         35.         36.\n 30.         27.         22.         40.         39.         27.86873699\n 33.12898535 35.2958243  35.         24.         34.         26.\n  4.         26.         27.         42.         20.         21.\n 21.         61.         57.         21.         26.         19.89558113\n 80.         51.         32.         38.83477484  9.         28.\n 32.         31.         41.         26.68916849 20.         24.\n  2.         29.56889809  0.75       48.         19.         56.\n 31.10838452 23.         27.94720616 18.         21.         26.3814249\n 18.         24.         27.94720616 32.         23.         58.\n 50.         40.         47.         36.         20.         32.\n 25.         27.51545426 43.         39.08817814 40.         31.\n 70.         31.         35.05181757 18.         24.5        18.\n 43.         36.         23.47643239 27.         20.         14.\n 60.         25.         14.         19.         18.         15.\n 31.          4.         29.56889809 25.         60.         52.\n 44.         19.89558113 49.         42.         18.         35.\n 18.         25.         26.         39.         45.         42.\n 22.         17.09991595 24.         49.8994093  48.         29.\n 52.         19.         38.         27.         27.30887391 33.\n  6.         17.         34.         50.         27.         20.\n 30.         19.89558113 25.         25.         29.         11.\n 35.05181757 23.         23.         28.5        48.         35.\n 27.94720616 27.94720616 38.42663175 36.         21.         24.\n 31.         70.         16.         30.         19.         31.\n  4.          6.         33.         23.         48.          0.67\n 28.         18.         34.         33.         24.33021696 41.\n 20.         36.         16.         51.         39.17490238 30.5\n 33.55117591 32.         24.         48.         57.         29.51820514\n 54.         18.         35.2958243   5.         19.89558113 43.\n 13.         17.         29.         16.1939502  25.         25.\n 18.          8.          1.         46.         35.2958243  16.\n 10.86986696 50.38328427 25.         39.         49.         31.\n 30.         30.         34.         31.         11.          0.42\n 27.         31.         39.         18.         39.         33.\n 26.         39.         35.          6.         30.5        38.83477484\n 23.         31.         43.         10.         52.         27.\n 38.         27.          2.         35.09787898 29.56889809  1.\n 35.2958243  62.         15.          0.83       22.87630686 23.\n 18.         39.         21.         30.70572678 32.         50.91095013\n 20.         16.         30.         34.5        17.         42.\n 10.86986696 35.         28.         43.96476448  4.         74.\n  9.         16.         44.         18.         45.         51.\n 24.         22.87630686 41.         21.         48.         10.86986696\n 24.         42.         27.         31.         25.97788916  4.\n 26.         47.         33.         47.         28.         15.\n 20.         19.         27.94720616 56.         25.         33.\n 22.         28.         25.         39.         27.         19.\n 16.1939502  26.         32.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-24b6b3e8bee5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mage_scale_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age_scaled'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage_scale_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfare_scale_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[22.         38.         26.         35.         35.         23.83895259\n 54.          2.         27.         14.          4.         58.\n 20.         39.         14.         55.          2.         32.06649305\n 31.         29.51820514 35.         34.         15.         28.\n  8.         38.         29.51820514 19.         22.38011324 27.94720616\n 40.         36.10804822 35.2958243  66.         28.         42.\n 22.87630686 21.         18.         14.         40.         27.\n 27.94720616  3.         19.         30.70572678 33.12898535 35.2958243\n 23.45968333 18.          7.         21.         49.         29.\n 65.         44.06483036 21.         28.5         5.         11.\n 22.         38.         45.          4.         41.20008848 17.09991595\n 29.         19.         17.         26.         32.         16.\n 21.         26.         32.         25.         27.94720616 30.70572678\n  0.83       30.         22.         29.         23.32262739 28.\n 17.         33.         16.         30.70572678 23.         24.\n 29.         20.         46.         26.         59.         30.70572678\n 71.         23.         34.         34.         28.         27.94720616\n 21.         33.         37.         28.         21.         27.51545426\n 38.         33.55117591 47.         14.5        22.         20.\n 17.         21.         70.5        29.         24.          2.\n 21.         30.70572678 32.5        32.5        54.         12.\n 35.2958243  24.         25.78337698 45.         33.         20.\n 47.         29.         25.         23.         19.         37.\n 16.         24.         25.34409583 22.         24.         19.\n 18.         19.         27.          9.         36.5        42.\n 51.         22.         55.5        40.5        29.78279613 51.\n 16.         30.         25.52340334 10.86986696 44.         40.\n 26.         17.          1.          9.         26.03188214 45.\n 49.5542756  28.         61.          4.          1.         21.\n 56.         18.          7.30954704 50.         30.         36.\n 10.86986696 31.71894048  9.          1.          4.         46.24976824\n 33.12898535 45.         40.         36.         32.         19.\n 19.          3.         44.         58.         35.2958243  42.\n 35.2958243  24.         28.         10.86986696 34.         45.5\n 18.          2.         32.         26.         16.         40.\n 24.         35.         22.         30.         31.09342985 31.\n 27.         42.         32.         30.         16.         27.\n 51.         27.94720616 38.         22.         19.         20.5\n 18.          7.30954704 35.         29.         59.          5.\n 24.         31.10838452 44.          8.         19.         33.\n 20.80015413 33.12898535 29.         22.         30.         44.\n 25.         24.         37.         54.         29.78279613 29.\n 62.         30.         41.         29.         34.62028571 30.\n 35.         50.         35.2958243   3.         52.         40.\n 35.2958243  36.         16.         25.         58.         35.\n 36.87489821 25.         41.         37.         35.2958243  63.\n 45.         35.05181757  7.         35.         65.         28.\n 16.         19.         57.74249226 33.         30.         22.\n 42.         22.         26.         19.         36.         24.\n 24.         41.20008848 23.5         2.         41.57487718 50.\n 35.2958243  23.31368333 19.         42.57451554 30.70572678  0.92\n 28.57888393 17.         30.         30.         24.         18.\n 26.         28.         43.         26.         24.         54.\n 31.         40.         22.         27.         30.         22.\n 10.86986696 36.         61.         36.         31.         16.\n 23.31368333 45.5        38.         16.         31.42587794 27.94720616\n 29.         41.         45.         45.          2.         24.\n 28.         25.         36.         24.         40.         26.68916849\n  3.         42.         23.         59.96916448 15.         25.\n 29.51820514 28.         22.         38.         22.38011324 22.38011324\n 40.         29.         45.         35.         33.12898535 30.\n 60.         22.87630686 35.2958243  24.         25.         18.\n 19.         22.          3.         31.94479345 22.         27.\n 20.         19.         42.          1.         32.         35.\n 27.94720616 18.          1.         36.         19.89558113 17.\n 36.         21.         28.         23.         24.         22.\n 31.         46.         23.         28.         39.         26.\n 21.         28.         20.         34.         51.          3.\n 21.          7.30954704 27.94720616 20.68131488 33.         35.05181757\n 44.         30.70572678 34.         18.         30.         10.\n 27.94720616 21.         29.         28.         18.         29.78279613\n 28.         19.         35.2958243  32.         28.         26.68916849\n 42.         17.         50.         14.         21.         24.\n 64.         31.         45.         20.         25.         28.\n 23.47643239  4.         13.         34.          5.         52.\n 36.         24.10899881 30.         49.         30.70572678 29.\n 65.         44.06001827 50.         35.2958243  48.         34.\n 47.         48.         30.70572678 38.         35.05181757 56.\n 19.89558113  0.75       29.78279613 38.         33.         23.\n 22.         44.65953056 34.         29.         22.          2.\n  9.         35.05181757 50.         63.         25.          7.30954704\n 35.         58.         30.          9.         24.10899881 21.\n 55.         71.         21.         18.19479484 54.         25.7554753\n 25.         24.         17.         21.         26.82073281 37.\n 16.         18.         33.         49.8994093  28.         26.\n 29.         30.70572678 36.         54.         24.         47.\n 34.         34.17374861 36.         32.         30.         22.\n 29.51820514 44.         22.87630686 40.5        50.         38.5155\n 39.         23.          2.         22.87630686 17.          8.01370298\n 30.          7.         45.         30.         24.33021696 22.\n 36.          9.         11.         32.         50.         64.\n 19.         32.79502428 33.          8.         17.         27.\n 26.3814249  22.         22.         62.         48.         38.5155\n 39.         36.         35.2958243  40.         28.         30.70572678\n 30.70572678 24.         19.         29.         22.87630686 32.\n 62.         53.         36.         35.2958243  16.         19.\n 34.         39.         20.07016773 32.         25.         39.\n 54.         36.         29.04293865 18.         47.         60.\n 22.         30.70572678 35.         52.         47.         27.45639428\n 37.         36.         24.40235514 49.         29.51820514 49.\n 24.         27.94720616 37.51615833 44.         35.         36.\n 30.         27.         22.         40.         39.         27.86873699\n 33.12898535 35.2958243  35.         24.         34.         26.\n  4.         26.         27.         42.         20.         21.\n 21.         61.         57.         21.         26.         19.89558113\n 80.         51.         32.         38.83477484  9.         28.\n 32.         31.         41.         26.68916849 20.         24.\n  2.         29.56889809  0.75       48.         19.         56.\n 31.10838452 23.         27.94720616 18.         21.         26.3814249\n 18.         24.         27.94720616 32.         23.         58.\n 50.         40.         47.         36.         20.         32.\n 25.         27.51545426 43.         39.08817814 40.         31.\n 70.         31.         35.05181757 18.         24.5        18.\n 43.         36.         23.47643239 27.         20.         14.\n 60.         25.         14.         19.         18.         15.\n 31.          4.         29.56889809 25.         60.         52.\n 44.         19.89558113 49.         42.         18.         35.\n 18.         25.         26.         39.         45.         42.\n 22.         17.09991595 24.         49.8994093  48.         29.\n 52.         19.         38.         27.         27.30887391 33.\n  6.         17.         34.         50.         27.         20.\n 30.         19.89558113 25.         25.         29.         11.\n 35.05181757 23.         23.         28.5        48.         35.\n 27.94720616 27.94720616 38.42663175 36.         21.         24.\n 31.         70.         16.         30.         19.         31.\n  4.          6.         33.         23.         48.          0.67\n 28.         18.         34.         33.         24.33021696 41.\n 20.         36.         16.         51.         39.17490238 30.5\n 33.55117591 32.         24.         48.         57.         29.51820514\n 54.         18.         35.2958243   5.         19.89558113 43.\n 13.         17.         29.         16.1939502  25.         25.\n 18.          8.          1.         46.         35.2958243  16.\n 10.86986696 50.38328427 25.         39.         49.         31.\n 30.         30.         34.         31.         11.          0.42\n 27.         31.         39.         18.         39.         33.\n 26.         39.         35.          6.         30.5        38.83477484\n 23.         31.         43.         10.         52.         27.\n 38.         27.          2.         35.09787898 29.56889809  1.\n 35.2958243  62.         15.          0.83       22.87630686 23.\n 18.         39.         21.         30.70572678 32.         50.91095013\n 20.         16.         30.         34.5        17.         42.\n 10.86986696 35.         28.         43.96476448  4.         74.\n  9.         16.         44.         18.         45.         51.\n 24.         22.87630686 41.         21.         48.         10.86986696\n 24.         42.         27.         31.         25.97788916  4.\n 26.         47.         33.         47.         28.         15.\n 20.         19.         27.94720616 56.         25.         33.\n 22.         28.         25.         39.         27.         19.\n 16.1939502  26.         32.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# 接下来我们要接着做一些数据预处理的工作，比如scaling，将一些变化幅度较大的特征化到[-1,1]之内\n",
    "# 这样可以加速logistic regression的收敛\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "age_scale_param = scaler.fit(df['Age'])\n",
    "df['Age_scaled'] = scaler.fit_transform(df['Age'], age_scale_param)\n",
    "fare_scale_param = scaler.fit(df['Fare'])\n",
    "df['Fare_scaled'] = scaler.fit_transform(df['Fare'], fare_scale_param)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=1e-06,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "    \n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来咱们对训练集和测试集做一样的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'age_scale_param' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-cbed45fcef5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummies_Cabin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummies_Embarked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummies_Sex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummies_Pclass\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Pclass'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Sex'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Ticket'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Cabin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Embarked'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age_scaled'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage_scale_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare_scaled'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfare_scale_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'age_scale_param' is not defined"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]\n",
    "predictedAges = rfr.predict(X)\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges\n",
    "\n",
    "data_test = set_Cabin_type(data_test)\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "df_test['Age_scaled'] = scaler.fit_transform(df_test['Age'], age_scale_param)\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(df_test['Fare'], fare_scale_param)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "predictions = clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         1\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         1\n",
       "19           911         1\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         0\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         0\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         0\n",
       "404         1296         1\n",
       "405         1297         1\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         1\n",
       "409         1301         1\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         1\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"logistic_regression_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=red>0.76555，恩，结果还不错。毕竟，这只是我们简单分析过后出的一个baseline系统嘛</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要判定一下当前模型所处状态(欠拟合or过拟合)\n",
    "\n",
    "<font color=red>有一个很可能发生的问题是，我们不断地做feature engineering，产生的特征越来越多，用这些特征去训练模型，会对我们的训练集拟合得越来越好，同时也可能在逐步丧失泛化能力，从而在待预测的数据上，表现不佳，也就是发生过拟合问题。<font><br>\n",
    "\n",
    "<font color=red>从另一个角度上说，如果模型在待预测的数据上表现不佳，除掉上面说的过拟合问题，也有可能是欠拟合问题，也就是说在训练集上，其实拟合的也不是那么好。<font><br>\n",
    "\n",
    "<font color=red>额，这个欠拟合和过拟合怎么解释呢。这么说吧：<font><br>\n",
    "\n",
    "1. <font color=red>过拟合就像是你班那个学数学比较刻板的同学，老师讲过的题目，一字不漏全记下来了，于是老师再出一样的题目，分分钟精确出结果。but数学考试，因为总是碰到新题目，所以成绩不咋地。<font>\n",
    "2. <font color=red>欠拟合就像是，咳咳，和博主level差不多的差生。连老师讲的练习题也记不住，于是连老师出一样题目复习的周测都做不好，考试更是可想而知了。<font>\n",
    "\n",
    "<font color=red>而在机器学习的问题上，对于过拟合和欠拟合两种情形。我们优化的方式是不同的。<font><br>\n",
    "\n",
    "<font color=red>对过拟合而言，通常以下策略对结果优化是有用的：<font><br>\n",
    "\n",
    "* <font color=red>做一下feature selection，挑出较好的feature的subset来做training\n",
    "* <font color=red>提供更多的数据，从而弥补原始数据的bias问题，学习到的model也会更准确\n",
    "\n",
    "<font color=red>而对于欠拟合而言，我们通常需要更多的feature，更复杂的模型来提高准确度。<font><br>\n",
    "\n",
    "<font color=red>著名的learning curve可以帮我们判定我们的模型现在所处的状态。我们以样本数为横坐标，训练和交叉验证集上的错误率作为纵坐标，两种状态分别如下两张图所示：过拟合(overfitting/high variace)，欠拟合(underfitting/high bias)<font><br>\n",
    "\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/high_variance.png?imageView/2/w/400/q/100)\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/high_bias.png?imageView/2/w/400/q/100)\n",
    "\n",
    "<font color=red>著名的learning curve可以帮我们判定我们的模型现在所处的状态。我们以样本数为横坐标，训练和交叉验证集上的错误率作为纵坐标，两种状态分别如下两张图所示：过拟合(overfitting/high variace)，欠拟合(underfitting/high bias)<font><br>\n",
    "\n",
    "<font color=red>我们也可以把错误率替换成准确率(得分)，得到另一种形式的learning curve(sklearn 里面是这么做的)。<font><br>\n",
    "\n",
    "<font color=red>回到我们的问题，我们用scikit-learn里面的learning_curve来帮我们分辨我们模型的状态。举个例子，这里我们一起画一下我们最先得到的baseline model的learning curve。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [86, 891]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-223b04462f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmidpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mplot_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu\"学习曲线\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-223b04462f4c>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[1;34m(estimator, title, X, y, ylim, cv, n_jobs, train_sizes, verbose, plot)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \"\"\"\n\u001b[0;32m     20\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(\n\u001b[1;32m---> 21\u001b[1;33m         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, error_score)\u001b[0m\n\u001b[0;32m    135\u001b[0m                          \"to exploit incremental learning\")\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;31m# Make a list since we will be iterating multiple times over the folds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [86, 891]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "# 用sklearn的learning_curve得到training_score和cv_score，使用matplotlib画出learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, \n",
    "                        train_sizes=np.linspace(.05, 1., 20), verbose=0, plot=True):\n",
    "    \"\"\"\n",
    "    画出data在某模型上的learning curve.\n",
    "    参数解释\n",
    "    ----------\n",
    "    estimator : 你用的分类器。\n",
    "    title : 表格的标题。\n",
    "    X : 输入的feature，numpy类型\n",
    "    y : 输入的target vector\n",
    "    ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点\n",
    "    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)\n",
    "    n_jobs : 并行的的任务数(默认1)\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(u\"训练样本数\")\n",
    "        plt.ylabel(u\"得分\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=u\"训练集上得分\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=u\"交叉验证集上得分\")\n",
    "    \n",
    "        plt.legend(loc=\"best\")\n",
    "        \n",
    "        plt.draw()\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    \n",
    "    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
    "    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n",
    "    return midpoint, diff\n",
    "\n",
    "plot_learning_curve(clf, u\"学习曲线\", X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/learning_curve.png?imageView/2/w/600/q/100)\n",
    "<font color=red>在实际数据上看，我们得到的learning curve没有理论推导的那么光滑哈，但是可以大致看出来，训练集和交叉验证集上的得分曲线走势还是符合预期的。<font><br>\n",
    "\n",
    "<font color=red>目前的曲线看来，我们的model并不处于overfitting的状态(overfitting的表现一般是训练集上得分高，而交叉验证集上要低很多，中间的gap比较大)。因此我们可以再做些feature engineering的工作，添加一些新产出的特征或者组合特征到模型中。<font><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>接下来，我们就该看看如何优化baseline系统了<br>\n",
    "我们还有些特征可以再挖掘挖掘<br><br>\n",
    "\n",
    "1. 比如说Name和Ticket两个属性被我们完整舍弃了(好吧，其实是一开始我们对于这种，每一条记录都是一个完全不同的值的属性，并没有很直接的处理方式)<br>\n",
    "2. 比如说，我们想想，年龄的拟合本身也未必是一件非常靠谱的事情<br>\n",
    "3. 另外，以我们的日常经验，小盆友和老人可能得到的照顾会多一些，这样看的话，年龄作为一个连续值，给一个固定的系数，似乎体现不出两头受照顾的实际情况，所以，说不定我们把年龄离散化，按区段分作类别属性会更合适一些<br>\n",
    "\n",
    "那怎么样才知道，哪些地方可以优化，哪些优化的方法是promising的呢？<br>\n",
    "是的<br><br>\n",
    "\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br><br>\n",
    "\n",
    "重要的事情说3编！！！<br>\n",
    "因为test.csv里面并没有Survived这个字段(好吧，这是废话，这明明就是我们要预测的结果)，我们无法在这份数据上评定我们算法在该场景下的效果。。。<br>\n",
    "我们通常情况下，这么做cross validation：把train.csv分成两部分，一部分用于训练我们需要的模型，另外一部分数据上看我们预测算法的效果。<br>\n",
    "我们可以用scikit-learn的cross_validation来完成这个工作</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>在此之前，咱们可以看看现在得到的模型的系数，因为系数和它们最终的判定能力强弱是正相关的</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"columns\":list(train_df.columns)[1:], \"coef\":list(clf.coef_.T)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "上面的系数和最后的结果是一个正相关的关系<br>\n",
    "我们先看看那些权重绝对值非常大的feature，在我们的模型上：<br>\n",
    "\n",
    "* Sex属性，如果是female会极大提高最后获救的概率，而male会很大程度拉低这个概率。\n",
    "* Pclass属性，1等舱乘客最后获救的概率会上升，而乘客等级为3会极大地拉低这个概率。\n",
    "* 有Cabin值会很大程度拉升最后获救概率(这里似乎能看到了一点端倪，事实上从最上面的有无Cabin记录的Survived分布图上看出，即使有Cabin记录的乘客也有一部分遇难了，估计这个属性上我们挖掘还不够)\n",
    "* Age是一个负相关，意味着在我们的模型里，年龄越小，越有获救的优先权(还得回原数据看看这个是否合理）\n",
    "* 有一个登船港口S会很大程度拉低获救的概率，另外俩港口压根就没啥作用(这个实际上非常奇怪，因为我们从之前的统计图上并没有看到S港口的获救率非常低，所以也许可以考虑把登船港口这个feature去掉试试)。\n",
    "* 船票Fare有小幅度的正相关(并不意味着这个feature作用不大，有可能是我们细化的程度还不够，举个例子，说不定我们得对它离散化，再分至各个乘客等级上？)\n",
    "\n",
    "噢啦，观察完了，我们现在有一些想法了，但是怎么样才知道，哪些优化的方法是promising的呢？<br>\n",
    "\n",
    "恩，要靠交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "# 简单看看打分情况\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "all_data = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "X = all_data.as_matrix()[:,1:]\n",
    "y = all_data.as_matrix()[:,0]\n",
    "print cross_validation.cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "\n",
    "# 分割数据\n",
    "split_train, split_cv = cross_validation.train_test_split(\u001d",
    "df, test_size=0.3, random_state=0)\n",
    "train_df = split_train.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "# 生成模型\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(train_df.as_matrix()[:,1:], train_df.as_matrix()[:,0])\n",
    "\n",
    "\n",
    "\n",
    "# 对cross validation数据进行预测\n",
    "\n",
    "cv_df = split_cv.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "predictions = clf.predict(cv_df.as_matrix()[:,1:])\n",
    "split_cv[ predictions != cv_df.as_matrix()[:,0] ].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 去除预测错误的case看原始dataframe数据\n",
    "#split_cv['PredictResult'] = predictions\n",
    "origin_data_train = pd.read_csv(\"Train.csv\")\n",
    "bad_cases = origin_data_train.loc[origin_data_train['PassengerId'].isin(split_cv[predictions != cv_df.as_matrix()[:,0]]['PassengerId'].values)]\n",
    "bad_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比bad case，我们仔细看看我们预测错的样本，到底是哪些特征有问题，咱们处理得还不够细？<br>\n",
    "\n",
    "我们随便列一些可能可以做的优化操作：<br>\n",
    "\n",
    "* Age属性不使用现在的拟合方式，而是根据名称中的『Mr』『Mrs』『Miss』等的平均值进行填充。\n",
    "* Age不做成一个连续值属性，而是使用一个步长进行离散化，变成离散的类目feature。\n",
    "* Cabin再细化一些，对于有记录的Cabin属性，我们将其分为前面的字母部分(我猜是位置和船层之类的信息) 和 后面的数字部分(应该是房间号，有意思的事情是，如果你仔细看看原始数据，你会发现，这个值大的情况下，似乎获救的可能性高一些)。\n",
    "* Pclass和Sex俩太重要了，我们试着用它们去组出一个组合属性来试试，这也是另外一种程度的细化。\n",
    "* 单加一个Child字段，Age<=12的，设为1，其余为0(你去看看数据，确实小盆友优先程度很高啊)\n",
    "* 如果名字里面有『Mrs』，而Parch>1的，我们猜测她可能是一个母亲，应该获救的概率也会提高，因此可以多加一个Mother字段，此种情况下设为1，其余情况下设为0\n",
    "* 登船港口可以考虑先去掉试试(Q和C本来就没权重，S有点诡异)\n",
    "* 把堂兄弟/兄妹 和 Parch 还有自己 个数加在一起组一个Family_size字段(考虑到大家族可能对最后的结果有影响)\n",
    "* Name是一个我们一直没有触碰的属性，我们可以做一些简单的处理，比如说男性中带某些字眼的(‘Capt’, ‘Don’, ‘Major’, ‘Sir’)可以统一到一个Title，女性也一样。\n",
    "\n",
    "大家接着往下挖掘，可能还可以想到更多可以细挖的部分。我这里先列这些了，然后我们可以使用手头上的”train_df”和”cv_df”开始试验这些feature engineering的tricks是否有效了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[data_train['Name'].str.contains(\"Major\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"Train.csv\")\n",
    "data_train['Sex_Pclass'] = data_train.Sex + \"_\" + data_train.Pclass.map(str)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]\n",
    "\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "data_train, rfr = set_missing_ages(data_train)\n",
    "data_train = set_Cabin_type(data_train)\n",
    "\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_train['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "age_scale_param = scaler.fit(df['Age'])\n",
    "df['Age_scaled'] = scaler.fit_transform(df['Age'], age_scale_param)\n",
    "fare_scale_param = scaler.fit(df['Fare'])\n",
    "df['Fare_scaled'] = scaler.fit_transform(df['Fare'], fare_scale_param)\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0\n",
    "data_test['Sex_Pclass'] = data_test.Sex + \"_\" + data_test.Pclass.map(str)\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]\n",
    "predictedAges = rfr.predict(X)\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges\n",
    "\n",
    "data_test = set_Cabin_type(data_test)\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_test['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "df_test['Age_scaled'] = scaler.fit_transform(df_test['Age'], age_scale_param)\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(df_test['Fare'], fare_scale_param)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "predictions = clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>一般做到后期，咱们要进行模型优化的方法就是模型融合啦<br>\n",
    "先解释解释啥叫模型融合哈，我们还是举几个例子直观理解一下好了。<br><br>\n",
    "\n",
    "大家都看过知识问答的综艺节目中，求助现场观众时候，让观众投票，最高的答案作为自己的答案的形式吧，每个人都有一个判定结果，最后我们相信答案在大多数人手里。<br>\n",
    "\n",
    "再通俗一点举个例子。你和你班某数学大神关系好，每次作业都『模仿』他的，于是绝大多数情况下，他做对了，你也对了。突然某一天大神脑子犯糊涂，手一抖，写错了一个数，于是…恩，你也只能跟着错了。 <br>\n",
    "我们再来看看另外一个场景，你和你班5个数学大神关系都很好，每次都把他们作业拿过来，对比一下，再『自己做』，那你想想，如果哪天某大神犯糊涂了，写错了，but另外四个写对了啊，那你肯定相信另外4人的是正确答案吧？<br>\n",
    "\n",
    "最简单的模型融合大概就是这么个意思，比如分类问题，当我们手头上有一堆在同一份数据集上训练得到的分类器(比如logistic regression，SVM，KNN，random forest，神经网络)，那我们让他们都分别去做判定，然后对结果做投票统计，取票数最多的结果为最后结果。<br>\n",
    "\n",
    "bingo，问题就这么完美的解决了。<br>\n",
    "\n",
    "模型融合可以比较好地缓解，训练过程中产生的过拟合问题，从而对于结果的准确度提升有一定的帮助。<br>\n",
    "\n",
    "话说回来，回到我们现在的问题。你看，我们现在只讲了logistic regression，如果我们还想用这个融合思想去提高我们的结果，我们该怎么做呢？<br>\n",
    "\n",
    "既然这个时候模型没得选，那咱们就在数据上动动手脚咯。大家想想，如果模型出现过拟合现在，一定是在我们的训练上出现拟合过度造成的对吧。<br>\n",
    "\n",
    "那我们干脆就不要用全部的训练集，每次取训练集的一个subset，做训练，这样，我们虽然用的是同一个机器学习算法，但是得到的模型却是不一样的；同时，因为我们没有任何一份子数据集是全的，因此即使出现过拟合，也是在子训练集上出现过拟合，而不是全体数据上，这样做一个融合，可能对最后的结果有一定的帮助。对，这就是常用的Bagging。<br>\n",
    "\n",
    "我们用scikit-learn里面的Bagging来完成上面的思路，过程非常简单。代码如下：<br><br><font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到BaggingRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "bagging_clf = BaggingRegressor(clf, n_estimators=10, max_samples=0.8, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1)\n",
    "bagging_clf.fit(X, y)\n",
    "\n",
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "predictions = bagging_clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"/Users/MLS/Downloads/logistic_regression_predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>下面是咱们用别的分类器解决这个问题的代码：</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import  DataFrame\n",
    "from patsy import dmatrices\n",
    "import string\n",
    "from operator import itemgetter\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split,StratifiedShuffleSplit,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "##Read configuration parameters\n",
    "\n",
    "train_file=\"train.csv\"\n",
    "MODEL_PATH=\"./\"\n",
    "test_file=\"test.csv\"\n",
    "SUBMISSION_PATH=\"./\"\n",
    "seed= 0\n",
    "\n",
    "print train_file,seed\n",
    "\n",
    "# 输出得分\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "#清理和处理数据\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if string.find(big_string, substring) != -1:\n",
    "            return substring\n",
    "    print big_string\n",
    "    return np.nan\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "enc=preprocessing.OneHotEncoder()\n",
    "\n",
    "def clean_and_munge_data(df):\n",
    "    #处理缺省值\n",
    "    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)\n",
    "    #处理一下名字，生成Title字段\n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "                'Don', 'Jonkheer']\n",
    "    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "\n",
    "    #处理特殊的称呼，全处理成mr, mrs, miss, master\n",
    "    def replace_titles(x):\n",
    "        title=x['Title']\n",
    "        if title in ['Mr','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "            return 'Mr'\n",
    "        elif title in ['Master']:\n",
    "            return 'Master'\n",
    "        elif title in ['Countess', 'Mme','Mrs']:\n",
    "            return 'Mrs'\n",
    "        elif title in ['Mlle', 'Ms','Miss']:\n",
    "            return 'Miss'\n",
    "        elif title =='Dr':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Mr'\n",
    "            else:\n",
    "                return 'Mrs'\n",
    "        elif title =='':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Master'\n",
    "            else:\n",
    "                return 'Miss'\n",
    "        else:\n",
    "            return title\n",
    "\n",
    "    df['Title']=df.apply(replace_titles, axis=1)\n",
    "\n",
    "    #看看家族是否够大，咳咳\n",
    "    df['Family_Size']=df['SibSp']+df['Parch']\n",
    "    df['Family']=df['SibSp']*df['Parch']\n",
    "\n",
    "\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==1),'Fare'] =np.median(df[df['Pclass'] == 1]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==2),'Fare'] =np.median( df[df['Pclass'] == 2]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==3),'Fare'] = np.median(df[df['Pclass'] == 3]['Fare'].dropna())\n",
    "\n",
    "    df['Gender'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "    df['AgeFill']=df['Age']\n",
    "    mean_ages = np.zeros(4)\n",
    "    mean_ages[0]=np.average(df[df['Title'] == 'Miss']['Age'].dropna())\n",
    "    mean_ages[1]=np.average(df[df['Title'] == 'Mrs']['Age'].dropna())\n",
    "    mean_ages[2]=np.average(df[df['Title'] == 'Mr']['Age'].dropna())\n",
    "    mean_ages[3]=np.average(df[df['Title'] == 'Master']['Age'].dropna())\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Miss') ,'AgeFill'] = mean_ages[0]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mrs') ,'AgeFill'] = mean_ages[1]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mr') ,'AgeFill'] = mean_ages[2]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Master') ,'AgeFill'] = mean_ages[3]\n",
    "\n",
    "    df['AgeCat']=df['AgeFill']\n",
    "    df.loc[ (df.AgeFill<=10) ,'AgeCat'] = 'child'\n",
    "    df.loc[ (df.AgeFill>60),'AgeCat'] = 'aged'\n",
    "    df.loc[ (df.AgeFill>10) & (df.AgeFill <=30) ,'AgeCat'] = 'adult'\n",
    "    df.loc[ (df.AgeFill>30) & (df.AgeFill <=60) ,'AgeCat'] = 'senior'\n",
    "\n",
    "    df.Embarked = df.Embarked.fillna('S')\n",
    "\n",
    "\n",
    "    df.loc[ df.Cabin.isnull()==True,'Cabin'] = 0.5\n",
    "    df.loc[ df.Cabin.isnull()==False,'Cabin'] = 1.5\n",
    "\n",
    "    df['Fare_Per_Person']=df['Fare']/(df['Family_Size']+1)\n",
    "\n",
    "    #Age times class\n",
    "\n",
    "    df['AgeClass']=df['AgeFill']*df['Pclass']\n",
    "    df['ClassFare']=df['Pclass']*df['Fare_Per_Person']\n",
    "\n",
    "\n",
    "    df['HighLow']=df['Pclass']\n",
    "    df.loc[ (df.Fare_Per_Person<8) ,'HighLow'] = 'Low'\n",
    "    df.loc[ (df.Fare_Per_Person>=8) ,'HighLow'] = 'High'\n",
    "\n",
    "\n",
    "\n",
    "    le.fit(df['Sex'] )\n",
    "    x_sex=le.transform(df['Sex'])\n",
    "    df['Sex']=x_sex.astype(np.float)\n",
    "\n",
    "    le.fit( df['Ticket'])\n",
    "    x_Ticket=le.transform( df['Ticket'])\n",
    "    df['Ticket']=x_Ticket.astype(np.float)\n",
    "\n",
    "    le.fit(df['Title'])\n",
    "    x_title=le.transform(df['Title'])\n",
    "    df['Title'] =x_title.astype(np.float)\n",
    "\n",
    "    le.fit(df['HighLow'])\n",
    "    x_hl=le.transform(df['HighLow'])\n",
    "    df['HighLow']=x_hl.astype(np.float)\n",
    "\n",
    "\n",
    "    le.fit(df['AgeCat'])\n",
    "    x_age=le.transform(df['AgeCat'])\n",
    "    df['AgeCat'] =x_age.astype(np.float)\n",
    "\n",
    "    le.fit(df['Embarked'])\n",
    "    x_emb=le.transform(df['Embarked'])\n",
    "    df['Embarked']=x_emb.astype(np.float)\n",
    "\n",
    "    df = df.drop(['PassengerId','Name','Age','Cabin'], axis=1) #remove Name,Age and PassengerId\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "#读取数据\n",
    "traindf=pd.read_csv(train_file)\n",
    "##清洗数据\n",
    "df=clean_and_munge_data(traindf)\n",
    "########################################formula################################\n",
    " \n",
    "formula_ml='Survived~Pclass+C(Title)+Sex+C(AgeCat)+Fare_Per_Person+Fare+Family_Size' \n",
    "\n",
    "y_train, x_train = dmatrices(formula_ml, data=df, return_type='dataframe')\n",
    "y_train = np.asarray(y_train).ravel()\n",
    "print y_train.shape,x_train.shape\n",
    "\n",
    "##选择训练和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2,random_state=seed)\n",
    "#初始化分类器\n",
    "clf=RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=5, min_samples_split=1,\n",
    "  min_samples_leaf=1, max_features='auto',    bootstrap=False, oob_score=False, n_jobs=1, random_state=seed,\n",
    "  verbose=0)\n",
    "\n",
    "###grid search找到最好的参数\n",
    "param_grid = dict( )\n",
    "##创建分类pipeline\n",
    "pipeline=Pipeline([ ('clf',clf) ])\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=3,scoring='accuracy',\\\n",
    "cv=StratifiedShuffleSplit(Y_train, n_iter=10, test_size=0.2, train_size=None, indices=None, \\\n",
    "random_state=seed, n_iterations=None)).fit(X_train, Y_train)\n",
    "# 对结果打分\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "report(grid_search.grid_scores_)\n",
    " \n",
    "print('-----grid search end------------')\n",
    "print ('on all train set')\n",
    "scores = cross_val_score(grid_search.best_estimator_, x_train, y_train,cv=3,scoring='accuracy')\n",
    "print scores.mean(),scores\n",
    "print ('on test set')\n",
    "scores = cross_val_score(grid_search.best_estimator_, X_test, Y_test,cv=3,scoring='accuracy')\n",
    "print scores.mean(),scores\n",
    "\n",
    "# 对结果打分\n",
    "\n",
    "print(classification_report(Y_train, grid_search.best_estimator_.predict(X_train) ))\n",
    "print('test data')\n",
    "print(classification_report(Y_test, grid_search.best_estimator_.predict(X_test) ))\n",
    "\n",
    "model_file=MODEL_PATH+'model-rf.pkl'\n",
    "joblib.dump(grid_search.best_estimator_, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
